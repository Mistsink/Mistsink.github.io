<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>预备知识 | 山野沉雾</title>
    <meta name="description" content="Mistsink">
    <link rel="preload stylesheet" href="/assets/style.98d5a8fa.css" as="style">
    <script type="module" src="/assets/app.dd0e3690.js"></script>
    <link rel="modulepreload" href="/assets/chunks/framework.13d73f51.js">
  <link rel="modulepreload" href="/assets/algorithm_预备知识.md.e2470309.lean.js">
  <script id="check-dark-light">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-1dcba787><!--[--><!--]--><!--[--><span tabindex="-1" data-v-238d3000></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-238d3000> Skip to content </a><!--]--><!----><header class="VPNav" data-v-1dcba787 data-v-5c175679><div class="VPNavBar" data-v-5c175679 data-v-17e08923><div class="container" data-v-17e08923><div class="title" data-v-17e08923><div class="VPNavBarTitle" data-v-17e08923 data-v-693bb139><a class="title" href="/" data-v-693bb139><!--[--><!--]--><!----><!--[-->山野沉雾<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-17e08923><div class="curtain" data-v-17e08923></div><div class="content-body" data-v-17e08923><!--[--><!--]--><!----><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-17e08923 data-v-b8edb269><span id="main-nav-aria-label" class="visually-hidden" data-v-b8edb269>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/TODO/" data-v-b8edb269 data-v-60c33da7 data-v-2fad7558><!--[-->TODO<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink active" href="/algorithm/" data-v-b8edb269 data-v-60c33da7 data-v-2fad7558><!--[-->算法<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/back-end/" data-v-b8edb269 data-v-60c33da7 data-v-2fad7558><!--[-->后端浅记<!--]--><!----></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/front-end/" data-v-b8edb269 data-v-60c33da7 data-v-2fad7558><!--[-->前端小集<!--]--><!----></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-17e08923 data-v-13a8872e><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-13a8872e data-v-0837fe51 data-v-30def96b><span class="check" data-v-30def96b><span class="icon" data-v-30def96b><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0837fe51><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0837fe51><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-17e08923 data-v-1350f3d6 data-v-31352670><!--[--><a class="VPSocialLink" href="https://github.com/Mistsink" target="_blank" rel="noopener" data-v-31352670 data-v-9b425669><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-17e08923 data-v-c48d6ea9 data-v-3bc73fac><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-3bc73fac><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-3bc73fac><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-3bc73fac><div class="VPMenu" data-v-3bc73fac data-v-ce2d1af9><!----><!--[--><!--[--><!----><div class="group" data-v-c48d6ea9><div class="item appearance" data-v-c48d6ea9><p class="label" data-v-c48d6ea9>Appearance</p><div class="appearance-action" data-v-c48d6ea9><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" aria-label="toggle dark mode" aria-checked="false" data-v-c48d6ea9 data-v-0837fe51 data-v-30def96b><span class="check" data-v-30def96b><span class="icon" data-v-30def96b><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-0837fe51><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-0837fe51><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="group" data-v-c48d6ea9><div class="item social-links" data-v-c48d6ea9><div class="VPSocialLinks social-links-list" data-v-c48d6ea9 data-v-31352670><!--[--><a class="VPSocialLink" href="https://github.com/Mistsink" target="_blank" rel="noopener" data-v-31352670 data-v-9b425669><svg role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-17e08923 data-v-2982dc1e><span class="container" data-v-2982dc1e><span class="top" data-v-2982dc1e></span><span class="middle" data-v-2982dc1e></span><span class="bottom" data-v-2982dc1e></span></span></button></div></div></div></div><!----></header><!----><!----><div class="VPContent" id="VPContent" data-v-1dcba787 data-v-b159cf03><div class="VPDoc has-aside" data-v-b159cf03 data-v-0fa51e1c><div class="container" data-v-0fa51e1c><div class="aside" data-v-0fa51e1c><div class="aside-curtain" data-v-0fa51e1c></div><div class="aside-container" data-v-0fa51e1c><div class="aside-content" data-v-0fa51e1c><div class="VPDocAside" data-v-0fa51e1c data-v-a89171d0><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" data-v-a89171d0 data-v-4e4e7f73><div class="content" data-v-4e4e7f73><div class="outline-marker" data-v-4e4e7f73></div><div class="outline-title" data-v-4e4e7f73>回到顶部⬆️</div><nav aria-labelledby="doc-outline-aria-label" data-v-4e4e7f73><span class="visually-hidden" id="doc-outline-aria-label" data-v-4e4e7f73> Table of Contents for current page </span><ul class="root" data-v-4e4e7f73 data-v-074c8da9><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-a89171d0></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-0fa51e1c><div class="content-container" data-v-0fa51e1c><!--[--><!--]--><main class="main" data-v-0fa51e1c><div style="position:relative;" class="vp-doc _algorithm_%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86" data-v-0fa51e1c><div><h1 id="预备知识" tabindex="-1">预备知识 <a class="header-anchor" href="#预备知识" aria-label="Permalink to &quot;预备知识&quot;">​</a></h1><blockquote><p>包含必备的 Python 操作、线性代数以及简单的数学知识</p></blockquote><h2 id="数据操作" tabindex="-1">数据操作 <a class="header-anchor" href="#数据操作" aria-label="Permalink to &quot;数据操作&quot;">​</a></h2><h3 id="节约内存" tabindex="-1">节约内存 <a class="header-anchor" href="#节约内存" aria-label="Permalink to &quot;节约内存&quot;">​</a></h3><blockquote><p>数据原地操作</p></blockquote><p><code>使用切片表示法将操作的结果分配给先前分配的数组</code></p><p>执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如<code>Y[:] = &lt;expression&gt;</code>。 为了说明这一点，我们首先创建一个新的矩阵<code>Z</code>，其形状与另一个<code>Y</code>相同， 使用<code>zeros_like</code>来分配一个全0的块。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">Z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">zeros_like</span><span style="color:#ABB2BF;">(Y)</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#ABB2BF;">Z[:] </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> X </span><span style="color:#56B6C2;">+</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">Z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">zeros_like</span><span style="color:#ABB2BF;">(Y)</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#ABB2BF;">Z[:] </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> X </span><span style="color:#56B6C2;">+</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"></span></code></pre></div><p>如果在后续计算中没有重复使用<code>X</code>， 我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">before </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X)</span></span>
<span class="line"><span style="color:#ABB2BF;">X </span><span style="color:#56B6C2;">+=</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X) </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> before</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#D19A66;">True</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">before </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X)</span></span>
<span class="line"><span style="color:#ABB2BF;">X </span><span style="color:#56B6C2;">+=</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X) </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> before</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#D19A66;">True</span></span>
<span class="line"></span></code></pre></div><h2 id="线性代数" tabindex="-1">线性代数 <a class="header-anchor" href="#线性代数" aria-label="Permalink to &quot;线性代数&quot;">​</a></h2><h3 id="张量算法的基本性质" tabindex="-1">张量算法的基本性质 <a class="header-anchor" href="#张量算法的基本性质" aria-label="Permalink to &quot;张量算法的基本性质&quot;">​</a></h3><h4 id="hadamard-product-⊙" tabindex="-1">Hadamard product ⊙ <a class="header-anchor" href="#hadamard-product-⊙" aria-label="Permalink to &quot;Hadamard product ⊙&quot;">​</a></h4><p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312111525665.png" alt="image-20230312111525665"></p><h4 id="非降维运算-重点在非降维" tabindex="-1">非降维运算（重点在非降维） <a class="header-anchor" href="#非降维运算-重点在非降维" aria-label="Permalink to &quot;非降维运算（重点在非降维）&quot;">​</a></h4><p><code>keepdims</code>参数与<code>广播机制</code></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># 求和</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> A.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">axis</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">1</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">keepdims</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[ </span><span style="color:#D19A66;">6</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">22</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">54</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">70</span><span style="color:#ABB2BF;">.]])</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 归一化</span></span>
<span class="line"><span style="color:#ABB2BF;">A </span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;"> sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[</span><span style="color:#D19A66;">0.0000</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.1667</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3333</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.5000</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.1818</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2273</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2727</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3182</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2105</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2368</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2632</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2895</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2222</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2407</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2593</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2778</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2286</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2429</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2571</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2714</span><span style="color:#ABB2BF;">]])</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># 求和</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> A.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">axis</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">1</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">keepdims</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[ </span><span style="color:#D19A66;">6</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">22</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">54</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">70</span><span style="color:#ABB2BF;">.]])</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 归一化</span></span>
<span class="line"><span style="color:#ABB2BF;">A </span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;"> sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[</span><span style="color:#D19A66;">0.0000</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.1667</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3333</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.5000</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.1818</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2273</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2727</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3182</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2105</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2368</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2632</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2895</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2222</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2407</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2593</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2778</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2286</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2429</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2571</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2714</span><span style="color:#ABB2BF;">]])</span></span>
<span class="line"></span></code></pre></div><h4 id="点积-dot-product" tabindex="-1">点积（Dot Product） <a class="header-anchor" href="#点积-dot-product" aria-label="Permalink to &quot;点积（Dot Product）&quot;">​</a></h4><p><code>torch.dot()</code></p><p>两向量各元素相乘再相加。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112320864.png" alt="image-20230312112320864"></p><h4 id="矩阵-向量-积-矩阵-向量" tabindex="-1">矩阵-向量 积（矩阵 * 向量） <a class="header-anchor" href="#矩阵-向量-积-矩阵-向量" aria-label="Permalink to &quot;矩阵-向量 积（矩阵 * 向量）&quot;">​</a></h4><p><code>torch.mv()</code></p><p>矩阵各行向量与向量的点积：</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112448685.png" alt="image-20230312112448685"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">A.shape, x.shape, torch.</span><span style="color:#61AFEF;">mv</span><span style="color:#ABB2BF;">(A, x)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># </span></span>
<span class="line"><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">5</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), </span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([ </span><span style="color:#D19A66;">14</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">62</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">86</span><span style="color:#ABB2BF;">., </span><span style="color:#D19A66;">110</span><span style="color:#ABB2BF;">.]))</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">A.shape, x.shape, torch.</span><span style="color:#61AFEF;">mv</span><span style="color:#ABB2BF;">(A, x)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># </span></span>
<span class="line"><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">5</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), </span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([ </span><span style="color:#D19A66;">14</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">62</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">86</span><span style="color:#ABB2BF;">., </span><span style="color:#D19A66;">110</span><span style="color:#ABB2BF;">.]))</span></span>
<span class="line"></span></code></pre></div><h4 id="矩阵相乘" tabindex="-1">矩阵相乘 <a class="header-anchor" href="#矩阵相乘" aria-label="Permalink to &quot;矩阵相乘&quot;">​</a></h4><blockquote><p>仅作为与<code>Hadamard 积</code>有区别的提醒</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112803028.png" alt="image-20230312112803028"></p><h4 id="范数" tabindex="-1">范数 <a class="header-anchor" href="#范数" aria-label="Permalink to &quot;范数&quot;">​</a></h4><blockquote><p>线性代数中最有用的一些运算符是<em>范数</em>（<code>norm</code>）。 非正式地说，向量的<em>范数</em>是表示一个向量有多大。 这里考虑的<em>大小</em>（<code>size</code>）概念不涉及维度，而是分量的大小。</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113045447.png" alt="image-20230312113045447"></p><h5 id="l2范数" tabindex="-1">L2范数 <a class="header-anchor" href="#l2范数" aria-label="Permalink to &quot;L2范数&quot;">​</a></h5><blockquote><p><em>L2范数</em>是向量元素平方和的平方根</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113144939.png" alt="image-20230312113144939"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">3.0</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">-</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">])</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(u)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(5.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">3.0</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">-</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">])</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(u)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(5.)</span></span>
<span class="line"></span></code></pre></div><h5 id="l1范数" tabindex="-1">L1范数 <a class="header-anchor" href="#l1范数" aria-label="Permalink to &quot;L1范数&quot;">​</a></h5><blockquote><p><em>L1范数</em>是向量元素的绝对值之和</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113346063.png" alt="image-20230312113346063"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">abs</span><span style="color:#ABB2BF;">(u).</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(7.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">abs</span><span style="color:#ABB2BF;">(u).</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(7.)</span></span>
<span class="line"></span></code></pre></div><h5 id="frobenius范数-针对矩阵" tabindex="-1"><em>Frobenius范数</em>（针对矩阵） <a class="header-anchor" href="#frobenius范数-针对矩阵" aria-label="Permalink to &quot;*Frobenius范数*（针对矩阵）&quot;">​</a></h5><blockquote><p>是矩阵元素平方和的平方根</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113539564.png" alt="image-20230312113539564"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">((</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">9</span><span style="color:#ABB2BF;">)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(6.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">((</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">9</span><span style="color:#ABB2BF;">)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(6.)</span></span>
<span class="line"></span></code></pre></div><h2 id="微积分" tabindex="-1">微积分 <a class="header-anchor" href="#微积分" aria-label="Permalink to &quot;微积分&quot;">​</a></h2><h3 id="梯度-gradient" tabindex="-1">梯度（gradient） <a class="header-anchor" href="#梯度-gradient" aria-label="Permalink to &quot;梯度（gradient）&quot;">​</a></h3><blockquote><p>就是函数的偏导；对于向量输入而言，偏导的输出也是一个向量：就是偏导向量，称为梯度向量。</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312114226705.png" alt="image-20230312114226705"></p><h2 id="自动微分" tabindex="-1">自动微分 <a class="header-anchor" href="#自动微分" aria-label="Permalink to &quot;自动微分&quot;">​</a></h2><blockquote><p>实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p></blockquote><h3 id="非标量变量的反向传播" tabindex="-1">非标量变量的反向传播 <a class="header-anchor" href="#非标量变量的反向传播" aria-label="Permalink to &quot;非标量变量的反向传播&quot;">​</a></h3><p>在<code>PyTorch</code>中有个简单的规定，不让张量对张量求导，只允许标量对张量求导。因此，目标量对一个非标量调用<code>backward()</code>，则需要传入一个<code>gradient参数</code>。传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">x </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">arange</span><span style="color:#ABB2BF;">(</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">requires_grad</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x  </span><span style="color:#7F848E;font-style:italic;"># y 是向量</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># y.backward()报错</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导数的和，那么给每个分量的梯度是1</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">gradient</span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">(</span><span style="color:#56B6C2;">len</span><span style="color:#ABB2BF;">(x)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数是对张量 y 进行 乘法运算，它就能输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 记 gradient 为张量 w，点积是向量维度的</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 若 y (亦即w) 是更高维度的张量，看作是 y 与 w 在每个维度上做点积，这样便输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 这就是将 对张量的反向传播 =&gt; 对标量的反向传播</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 该例中等价于: y.sum().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导的平均</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones_like</span><span style="color:#ABB2BF;">(x)</span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;">x.</span><span style="color:#61AFEF;">numel</span><span style="color:#ABB2BF;">())</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># === y.mean().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数表征的是 分配不同的权重 给各分量。因为不用的值的梯度对函数结果的影响程度可能不同。</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">x </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">arange</span><span style="color:#ABB2BF;">(</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">requires_grad</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x  </span><span style="color:#7F848E;font-style:italic;"># y 是向量</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># y.backward()报错</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导数的和，那么给每个分量的梯度是1</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">gradient</span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">(</span><span style="color:#56B6C2;">len</span><span style="color:#ABB2BF;">(x)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数是对张量 y 进行 乘法运算，它就能输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 记 gradient 为张量 w，点积是向量维度的</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 若 y (亦即w) 是更高维度的张量，看作是 y 与 w 在每个维度上做点积，这样便输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 这就是将 对张量的反向传播 =&gt; 对标量的反向传播</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 该例中等价于: y.sum().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导的平均</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones_like</span><span style="color:#ABB2BF;">(x)</span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;">x.</span><span style="color:#61AFEF;">numel</span><span style="color:#ABB2BF;">())</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># === y.mean().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数表征的是 分配不同的权重 给各分量。因为不用的值的梯度对函数结果的影响程度可能不同。</span></span>
<span class="line"></span></code></pre></div><h3 id="分离计算" tabindex="-1">分离计算 <a class="header-anchor" href="#分离计算" aria-label="Permalink to &quot;分离计算&quot;">​</a></h3><p><code>[tensor].detach()</code></p><p>将某些计算移动到记录的计算图之外。</p><p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 <strong>换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。</strong></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> y.</span><span style="color:#61AFEF;">detach</span><span style="color:#ABB2BF;">()  </span><span style="color:#7F848E;font-style:italic;"># 分离操作</span></span>
<span class="line"><span style="color:#ABB2BF;">z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> u </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;">z.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> u</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> </span><span style="color:#D19A66;">2</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> y.</span><span style="color:#61AFEF;">detach</span><span style="color:#ABB2BF;">()  </span><span style="color:#7F848E;font-style:italic;"># 分离操作</span></span>
<span class="line"><span style="color:#ABB2BF;">z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> u </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;">z.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> u</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> </span><span style="color:#D19A66;">2</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span></code></pre></div><h2 id="概率" tabindex="-1">概率 <a class="header-anchor" href="#概率" aria-label="Permalink to &quot;概率&quot;">​</a></h2><h3 id="概率论公理" tabindex="-1">概率论公理 <a class="header-anchor" href="#概率论公理" aria-label="Permalink to &quot;概率论公理&quot;">​</a></h3><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324125352715.png" alt="image-20230324125352715"></p><h3 id="处理多个随机变量" tabindex="-1">处理多个随机变量 <a class="header-anchor" href="#处理多个随机变量" aria-label="Permalink to &quot;处理多个随机变量&quot;">​</a></h3><h4 id="联合概率" tabindex="-1">联合概率 <a class="header-anchor" href="#联合概率" aria-label="Permalink to &quot;联合概率&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130537817.png" alt="image-20230324130537817"></p><h4 id="条件概率" tabindex="-1">条件概率 <a class="header-anchor" href="#条件概率" aria-label="Permalink to &quot;条件概率&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130621959.png" alt="image-20230324130621959"></p><h4 id="贝叶斯定理" tabindex="-1">贝叶斯定理 <a class="header-anchor" href="#贝叶斯定理" aria-label="Permalink to &quot;贝叶斯定理&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130755791.png" alt="image-20230324130755791"></p><h4 id="边际化" tabindex="-1">边际化 <a class="header-anchor" href="#边际化" aria-label="Permalink to &quot;边际化&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324131036973.png" alt="image-20230324131036973"></p><h4 id="独立性" tabindex="-1">独立性 <a class="header-anchor" href="#独立性" aria-label="Permalink to &quot;独立性&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324131429633.png" alt="image-20230324131429633"></p><h4 id="期望与方差" tabindex="-1">期望与方差 <a class="header-anchor" href="#期望与方差" aria-label="Permalink to &quot;期望与方差&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324133554377.png" alt="image-20230324133554377"></p></div></div></main><!--[--><!--]--><footer class="VPDocFooter" data-v-0fa51e1c data-v-46bc534f><div class="edit-info" data-v-46bc534f><!----><div class="last-updated" data-v-46bc534f><p class="VPLastUpdated" data-v-46bc534f data-v-ee12f3d4>最近更新时间: <time datetime="2023-03-24T11:35:38.000Z" data-v-ee12f3d4></time></p></div><div class="pageview-cnt" data-v-46bc534f> 浏览量:<span class="waline-pageview-count" data-v-46bc534f></span></div></div><div class="prev-next" data-v-46bc534f><div class="pager" data-v-46bc534f><a class="pager-link prev" href="/algorithm/%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" data-v-46bc534f><span class="desc" data-v-46bc534f>上一篇</span><span class="title" data-v-46bc534f>线性神经网络</span></a></div><div class="pager center" data-v-46bc534f><a class="pager-link" href="/algorithm/" data-v-46bc534f><span class="list-desc" data-v-46bc534f>文章列表</span></a></div><div class="has-prev pager" data-v-46bc534f><!----></div></div></footer><!--[--><!--[--><div data-waline class="waline" data-v-1dcba787><!--v-if--><div class="wl-comment"><!--v-if--><div class="wl-panel"><div class="wl-header item3"><!--[--><div class="wl-header-item"><label for="wl-nick">NickName</label><input id="wl-nick" class="wl-input wl-nick" name="nick" type="text" value></div><div class="wl-header-item"><label for="wl-mail">E-Mail</label><input id="wl-mail" class="wl-input wl-mail" name="mail" type="email" value></div><div class="wl-header-item"><label for="wl-link">Website</label><input id="wl-link" class="wl-input wl-link" name="link" type="text" value></div><!--]--></div><textarea id="wl-edit" class="wl-editor" placeholder="Comment here..."></textarea><div class="wl-preview" style="display:none;"><hr><h4>Preview:</h4><div class="wl-content"></div></div><div class="wl-footer"><div class="wl-actions"><a href="https://guides.github.com/features/mastering-markdown/" title="Markdown Guide" aria-label="Markdown is supported" class="wl-action" target="_blank" rel="noopener noreferrer"><svg width="16" height="16" ariaHidden="true"><path d="M14.85 3H1.15C.52 3 0 3.52 0 4.15v7.69C0 12.48.52 13 1.15 13h13.69c.64 0 1.15-.52 1.15-1.15v-7.7C16 3.52 15.48 3 14.85 3zM9 11H7V8L5.5 9.92 4 8v3H2V5h2l1.5 2L7 5h2v6zm2.99.5L9.5 8H11V5h2v3h1.5l-2.51 3.5z" fill="currentColor"></path></svg></a><button type="button" class="wl-action" title="Emoji" style="display:none;"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M563.2 463.3 677 540c1.7 1.2 3.7 1.8 5.8 1.8.7 0 1.4-.1 2-.2 2.7-.5 5.1-2.1 6.6-4.4l25.3-37.8c1.5-2.3 2.1-5.1 1.6-7.8s-2.1-5.1-4.4-6.6l-73.6-49.1 73.6-49.1c2.3-1.5 3.9-3.9 4.4-6.6.5-2.7 0-5.5-1.6-7.8l-25.3-37.8a10.1 10.1 0 0 0-6.6-4.4c-.7-.1-1.3-.2-2-.2-2.1 0-4.1.6-5.8 1.8l-113.8 76.6c-9.2 6.2-14.7 16.4-14.7 27.5.1 11 5.5 21.3 14.7 27.4zM387 348.8h-45.5c-5.7 0-10.4 4.7-10.4 10.4v153.3c0 5.7 4.7 10.4 10.4 10.4H387c5.7 0 10.4-4.7 10.4-10.4V359.2c0-5.7-4.7-10.4-10.4-10.4zm333.8 241.3-41-20a10.3 10.3 0 0 0-8.1-.5c-2.6.9-4.8 2.9-5.9 5.4-30.1 64.9-93.1 109.1-164.4 115.2-5.7.5-9.9 5.5-9.5 11.2l3.9 45.5c.5 5.3 5 9.5 10.3 9.5h.9c94.8-8 178.5-66.5 218.6-152.7 2.4-5 .3-11.2-4.8-13.6zm186-186.1c-11.9-42-30.5-81.4-55.2-117.1-24.1-34.9-53.5-65.6-87.5-91.2-33.9-25.6-71.5-45.5-111.6-59.2-41.2-14-84.1-21.1-127.8-21.1h-1.2c-75.4 0-148.8 21.4-212.5 61.7-63.7 40.3-114.3 97.6-146.5 165.8-32.2 68.1-44.3 143.6-35.1 218.4 9.3 74.8 39.4 145 87.3 203.3.1.2.3.3.4.5l36.2 38.4c1.1 1.2 2.5 2.1 3.9 2.6 73.3 66.7 168.2 103.5 267.5 103.5 73.3 0 145.2-20.3 207.7-58.7 37.3-22.9 70.3-51.5 98.1-85 27.1-32.7 48.7-69.5 64.2-109.1 15.5-39.7 24.4-81.3 26.6-123.8 2.4-43.6-2.5-87-14.5-129zm-60.5 181.1c-8.3 37-22.8 72-43 104-19.7 31.1-44.3 58.6-73.1 81.7-28.8 23.1-61 41-95.7 53.4-35.6 12.7-72.9 19.1-110.9 19.1-82.6 0-161.7-30.6-222.8-86.2l-34.1-35.8c-23.9-29.3-42.4-62.2-55.1-97.7-12.4-34.7-18.8-71-19.2-107.9-.4-36.9 5.4-73.3 17.1-108.2 12-35.8 30-69.2 53.4-99.1 31.7-40.4 71.1-72 117.2-94.1 44.5-21.3 94-32.6 143.4-32.6 49.3 0 97 10.8 141.8 32 34.3 16.3 65.3 38.1 92 64.8 26.1 26 47.5 56 63.6 89.2 16.2 33.2 26.6 68.5 31 105.1 4.6 37.5 2.7 75.3-5.6 112.3z" fill="currentColor"></path></svg></button><button type="button" class="wl-action" title="GIF"><svg width="24" height="24" fill="currentcolor" viewBox="0 0 24 24"><path style="transform: translateY(0.5px)" d="M18.968 10.5H15.968V11.484H17.984V12.984H15.968V15H14.468V9H18.968V10.5V10.5ZM8.984 9C9.26533 9 9.49967 9.09367 9.687 9.281C9.87433 9.46833 9.968 9.70267 9.968 9.984V10.5H6.499V13.5H8.468V12H9.968V14.016C9.968 14.2973 9.87433 14.5317 9.687 14.719C9.49967 14.9063 9.26533 15 8.984 15H5.984C5.70267 15 5.46833 14.9063 5.281 14.719C5.09367 14.5317 5 14.2973 5 14.016V9.985C5 9.70367 5.09367 9.46933 5.281 9.282C5.46833 9.09467 5.70267 9.001 5.984 9.001H8.984V9ZM11.468 9H12.968V15H11.468V9V9Z"></path><path d="M18.5 3H5.75C3.6875 3 2 4.6875 2 6.75V18C2 20.0625 3.6875 21.75 5.75 21.75H18.5C20.5625 21.75 22.25 20.0625 22.25 18V6.75C22.25 4.6875 20.5625 3 18.5 3ZM20.75 18C20.75 19.2375 19.7375 20.25 18.5 20.25H5.75C4.5125 20.25 3.5 19.2375 3.5 18V6.75C3.5 5.5125 4.5125 4.5 5.75 4.5H18.5C19.7375 4.5 20.75 5.5125 20.75 6.75V18Z"></path></svg></button><input id="wl-image-upload" class="upload" type="file" accept=".png,.jpg,.jpeg,.webp,.bmp,.gif"><label for="wl-image-upload" class="wl-action" title="Upload Image"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M784 112H240c-88 0-160 72-160 160v480c0 88 72 160 160 160h544c88 0 160-72 160-160V272c0-88-72-160-160-160zm96 640c0 52.8-43.2 96-96 96H240c-52.8 0-96-43.2-96-96V272c0-52.8 43.2-96 96-96h544c52.8 0 96 43.2 96 96v480z" fill="currentColor"></path><path d="M352 480c52.8 0 96-43.2 96-96s-43.2-96-96-96-96 43.2-96 96 43.2 96 96 96zm0-128c17.6 0 32 14.4 32 32s-14.4 32-32 32-32-14.4-32-32 14.4-32 32-32zm462.4 379.2-3.2-3.2-177.6-177.6c-25.6-25.6-65.6-25.6-91.2 0l-80 80-36.8-36.8c-25.6-25.6-65.6-25.6-91.2 0L200 728c-4.8 6.4-8 14.4-8 24 0 17.6 14.4 32 32 32 9.6 0 16-3.2 22.4-9.6L380.8 640l134.4 134.4c6.4 6.4 14.4 9.6 24 9.6 17.6 0 32-14.4 32-32 0-9.6-4.8-17.6-9.6-24l-52.8-52.8 80-80L769.6 776c6.4 4.8 12.8 8 20.8 8 17.6 0 32-14.4 32-32 0-8-3.2-16-8-20.8z" fill="currentColor"></path></svg></label><button type="button" class="wl-action" title="Preview"><svg viewBox="0 0 1024 1024" width="24" height="24"><path d="M710.816 654.301c70.323-96.639 61.084-230.578-23.705-314.843-46.098-46.098-107.183-71.109-172.28-71.109-65.008 0-126.092 25.444-172.28 71.109-45.227 46.098-70.756 107.183-70.756 172.106 0 64.923 25.444 126.007 71.194 172.106 46.099 46.098 107.184 71.109 172.28 71.109 51.414 0 100.648-16.212 142.824-47.404l126.53 126.006c7.058 7.06 16.297 10.979 26.406 10.979 10.105 0 19.343-3.919 26.402-10.979 14.467-14.467 14.467-38.172 0-52.723L710.816 654.301zm-315.107-23.265c-65.88-65.88-65.88-172.54 0-238.42 32.069-32.07 74.245-49.149 119.471-49.149 45.227 0 87.407 17.603 119.472 49.149 65.88 65.879 65.88 172.539 0 238.42-63.612 63.178-175.242 63.178-238.943 0zm0 0" fill="currentColor"></path><path d="M703.319 121.603H321.03c-109.8 0-199.469 89.146-199.469 199.38v382.034c0 109.796 89.236 199.38 199.469 199.38h207.397c20.653 0 37.384-16.645 37.384-37.299 0-20.649-16.731-37.296-37.384-37.296H321.03c-68.582 0-124.352-55.77-124.352-124.267V321.421c0-68.496 55.77-124.267 124.352-124.267h382.289c68.582 0 124.352 55.771 124.352 124.267V524.72c0 20.654 16.736 37.299 37.385 37.299 20.654 0 37.384-16.645 37.384-37.299V320.549c-.085-109.8-89.321-198.946-199.121-198.946zm0 0" fill="currentColor"></path></svg></button></div><div class="wl-info"><div class="wl-text-number">0 <!--v-if-->  Words</div><button type="button" class="wl-btn">Login</button><button type="submit" class="primary wl-btn" title="Cmd|Ctrl + Enter"><!--[-->Submit<!--]--></button></div><div class="wl-gif-popup"><input type="text" placeholder="Search GIF"><div class="wl-gallery" style="gap:6px;"><!--[--><!--]--></div><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div></div><div class="wl-emoji-popup"><!--[--><!--]--><!--v-if--></div></div></div><!--v-if--></div><div class="wl-meta-head"><div class="wl-count"><!--v-if--> Comments</div><ul class="wl-sort"><!--[--><li class="active">Latest</li><li class="">Oldest</li><li class="">Hottest</li><!--]--></ul></div><div class="wl-cards"><!--[--><!--]--></div><!--[--><div class="wl-loading"><svg width="30" height="30" viewBox="0 0 100 100" preserveAspectRatio="xMidYMid"><circle cx="50" cy="50" fill="none" stroke="currentColor" strokeWidth="4" r="40" stroke-dasharray="85 30"><animateTransform attributeName="transform" type="rotate" repeatCount="indefinite" dur="1s" values="0 50 50;360 50 50" keyTimes="0;1"></animateTransform></circle></svg></div><!--]--><div class="wl-power"> Powered by <a href="https://github.com/walinejs/waline" target="_blank" rel="noopener noreferrer"> Waline </a> v2.14.9</div></div><!--]--><!--]--></div></div></div></div></div><footer class="VPFooter" data-v-1dcba787 data-v-822cfb09><div class="container" data-v-822cfb09><!----><p class="copyright" data-v-822cfb09>Copyright © 2022-present Mistsink</p></div></footer><!--[--><!--]--></div></div>
    <script>__VP_HASH_MAP__ = JSON.parse("{\"algorithm_预备知识.md\":\"e2470309\",\"back-end_mysql45讲-小记.md\":\"7853a8bc\",\"essay_teachers-info.md\":\"a433bce6\",\"front-end_koa浅读.md\":\"01cb8f2a\",\"todo_index.md\":\"06e4b010\",\"algorithm_线性神经网络.md\":\"aec022d8\"}")
__VP_SITE_DATA__ = JSON.parse("{\"lang\":\"zh-CN\",\"dir\":\"ltr\",\"title\":\"山野沉雾\",\"description\":\"Mistsink\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"TODO\",\"link\":\"/TODO/\",\"activeMatch\":\"/TODO/\"},{\"text\":\"算法\",\"link\":\"/algorithm/\",\"activeMatch\":\"/algorithm/\"},{\"text\":\"后端浅记\",\"link\":\"/back-end/\",\"activeMatch\":\"/back-end/\"},{\"text\":\"前端小集\",\"link\":\"/front-end/\",\"activeMatch\":\"/front-end/\"}],\"sidebar\":{\"/TODO/\":[{\"text\":\"TODO\",\"items\":[]}],\"/algorithm/\":[{\"text\":\"算法\",\"items\":[{\"text\":\"线性神经网络\",\"link\":\"/algorithm/线性神经网络\",\"preview\":\"我们从经典算法-线性神经网络开始，介绍神经网络的基 ...\",\"createTime\":\"2023/3/24\"},{\"text\":\"预备知识\",\"link\":\"/algorithm/预备知识\",\"preview\":\"要学习深度学习，首先需要先掌握一些基本技能。 ...\",\"createTime\":\"2023/3/17\"}]}],\"/back-end/\":[{\"text\":\"后端浅记\",\"items\":[{\"text\":\"MySQL45讲 - 小记\",\"link\":\"/back-end/MySQL45讲-小记\",\"preview\":\"MySQL原理相关的阅读笔记 ...\",\"createTime\":\"2023/2/19\"}]}],\"/front-end/\":[{\"text\":\"前端小集\",\"items\":[{\"text\":\"koa浅读\",\"link\":\"/front-end/koa浅读\",\"preview\":\"直奔主题，直接 git clone，创建测试样例来 ...\",\"createTime\":\"2022/10/31\"}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/Mistsink\"}],\"outline\":{\"level\":[1,6],\"label\":\"回到顶部⬆️\"},\"outlineBadges\":true,\"docFooter\":{\"prev\":\"上一篇\",\"next\":\"下一篇\"},\"footer\":{\"message\":\"\",\"copyright\":\"Copyright © 2022-present Mistsink\"},\"lastUpdatedText\":\"最近更新时间\",\"sidebarMenuLabel\":\"文章列表\",\"returnToTopLabel\":\"回到顶部\"},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":true}")</script>
    
  </body>
</html>