---
layout: doc
title: 多层感知机
createTime: 2023/3/26
preview: 我们将第一次介绍真正的深度网络。 最简单的深度网络称为多层感知机。
---

# 多层感知机

> 最简单的深度网络称为*多层感知机*。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。 当我们训练容量较大的模型时，我们面临着*过拟合*的风险。 因此，本章将从基本的概念介绍开始讲起，包括*过拟合*、*欠拟合*和模型选择。 为了解决这些问题，本章将介绍*权重衰减*和*暂退法*等正则化技术。 我们还将讨论数值稳定性和参数初始化相关的问题， 这些问题是成功训练深度网络的关键。 

## 隐藏层

> 仿射变换中的*线性*是一个很强的假设。

每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。

![image-20230326202929640](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326202929640.png)

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后***对每个隐藏单元应用非线性的激活函数（activation function）σ。 激活函数的输出（例如，σ(⋅)）被称为活性值（activations）。*** 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
$$
\begin{aligned}&\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)})\\ &\mathbf{O}=\mathbf{H}\mathbf{W}{}^{(2)}+\mathbf{b}{}^{(2)}\end{aligned}
$$
> 通用近似定理：
>
> 多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。
>
> 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。

## 激活函数

激活函数是一种在神经网络中广泛使用的非线性函数，用于将神经网络中的输入信号进行变换，并输出给下一层神经元或输出层。

通常情况下，激活函数应该满足以下几个条件：

1. **非线性**：激活函数应该是非线性的，能够引入更多的非线性因素，从而提高网络的表达能力。
2. **可导**：激活函数应该是可导的，这样可以方便地使用梯度下降等优化算法进行网络训练。
3. **单调性**：激活函数应该具有单调性，这样可以保证网络训练过程中的误差函数具有唯一最小值，从而提高训练效果。
4. **有界性**：激活函数应该具有有界性，这样可以防止梯度爆炸或消失等问题，从而保证网络的稳定性。

### ReLU

最受欢迎的激活函数是`修正线性单元（Rectified linear unit，ReLU）`， 因为它实现简单，同时在各种预测任务中表现良好。

给定元素x，ReLU函数被定义为该元素与0的最大值：

$$
{ReLU}(x)=\max(x,0)
$$
${ReLU}(x)=\max(x,0)$