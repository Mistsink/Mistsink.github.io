---
layout: doc
title: 多层感知机
createTime: 2023/3/26
preview: 我们将第一次介绍真正的深度网络。 最简单的深度网络称为多层感知机。
---

# 多层感知机

> 最简单的深度网络称为*多层感知机*。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。 当我们训练容量较大的模型时，我们面临着*过拟合*的风险。 因此，本章将从基本的概念介绍开始讲起，包括*过拟合*、*欠拟合*和模型选择。 为了解决这些问题，本章将介绍*权重衰减*和*暂退法*等正则化技术。 我们还将讨论数值稳定性和参数初始化相关的问题， 这些问题是成功训练深度网络的关键。 

## 隐藏层

> 仿射变换中的*线性*是一个很强的假设。

每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。

![image-20230326202929640](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326202929640.png)

为了发挥多层架构的潜力， 我们还需要一个额外的关键要素： 在仿射变换之后***对每个隐藏单元应用非线性的激活函数（activation function）σ。 激活函数的输出（例如，σ(⋅)）被称为活性值（activations）。*** 一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：
$$
\begin{aligned}&\mathbf{H}=\sigma(\mathbf{X}\mathbf{W}^{(1)}+\mathbf{b}^{(1)})\\ &\mathbf{O}=\mathbf{H}\mathbf{W}{}^{(2)}+\mathbf{b}{}^{(2)}\end{aligned}
$$
> 通用近似定理：
>
> 多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。
>
> 事实上，通过使用更深（而不是更广）的网络，我们可以更容易地逼近许多函数。

## 激活函数

激活函数是一种在神经网络中广泛使用的非线性函数，用于将神经网络中的输入信号进行变换，并输出给下一层神经元或输出层。

通常情况下，激活函数应该满足以下几个条件：

1. **非线性**：激活函数应该是非线性的，能够引入更多的非线性因素，从而提高网络的表达能力。
2. **可导**：激活函数应该是可导的，这样可以方便地使用梯度下降等优化算法进行网络训练。
3. **单调性**：激活函数应该具有单调性，这样可以保证网络训练过程中的误差函数具有唯一最小值，从而提高训练效果。
4. **有界性**：激活函数应该具有有界性，这样可以防止梯度爆炸或消失等问题，从而保证网络的稳定性。

### ReLU

最受欢迎的激活函数是`修正线性单元（Rectified linear unit，ReLU）`， 因为它实现简单，同时在各种预测任务中表现良好。

给定元素x，ReLU函数被定义为该元素与0的最大值：

$$
{ReLU}(x)=\max(x,0)
$$
![image-20230327121552478](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327121552478.png)

导数：

- 输入为负：导数为 0
- 输入为正：导数为 1
- 输入为 0：导数不存在，默认使用左侧导数

![image-20230327122153580](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327122153580.png)

> 使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。 这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题（稍后将详细介绍）。

> 注意，ReLU函数有许多变体，包括*参数化ReLU*（Parameterized ReLU，*pReLU*） 函数 ([He *et al.*, 2015](https://zh.d2l.ai/chapter_references/zreferences.html#id59))。 该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：
> $$
> \text{pReLU}(x)=\max(0,x)+\alpha\min(0,x)
> $$

### Sigmoid

对于一个定义域在R中的输入， *sigmoid函数*将输入变换为区间(0, 1)上的输出。 因此，`sigmoid`通常称为`挤压函数（squashing function）`：它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：
$$
\text{sigmod}(x)=\dfrac{1}{1+\exp(-x)}
$$
它是一个平滑的、可微的阈值单元近似。

![image-20230327123011469](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327123011469.png)

导数：
$$
\dfrac{d}{dx}\text{signoid}(x)=\dfrac{\exp(-x)}{(1+\exp(-x))^2}=\text{signoid}(x)\left(1-\text{sigmod}(x)\right)
$$
![image-20230327123549840](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327123549840.png)

### Tanh

与`sigmoid`函数类似， `tanh(双曲正切)`函数也能将其输入压缩转换到区间(-1, 1)上。 `tanh`函数的公式如下：
$$
\tanh(x)=\dfrac{1-\exp(-2x)}{1+\exp(-2x)}
$$
注意，当输入在0附近时，`tanh`函数接近线性变换。 函数的形状类似于`sigmoid`函数， 不同的是`tanh`函数关于坐标系原点中心对称。

![image-20230327123857670](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327123857670.png)

导数：
$$
\dfrac{d}{dx}\tanh(x)=1-\tanh^2(x)
$$
![image-20230327124154784](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230327124154784.png)

## 模型选择、欠拟合、过拟合

> 将模型在训练数据上拟合的比在潜在分布中更接近的现象称为`过拟合（overfitting）`， 用于对抗过拟合的技术称为`正则化（regularization）`。

## 权重衰减

在训练参数化机器学习模型时， *权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为L2*正则化*。

一种简单的方法是通过线性函数 $f(\mathbf{x})=\mathbf{w}^{\top}\mathbf{x}$ 中的权重向量的某个范数来度量其复杂性。要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标*最小化训练标签上的预测损失*， 调整为*最小化预测损失和惩罚项之和*。

 L2正则化线性模型构成经典的*岭回归*（ridge regression）算法， L1正则化线性回归是统计学中类似的基本模型， 通常被称为*套索回归*（lasso regression）。 使用L2范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，L1惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为*特征选择*（feature selection），这可能是其他场景下需要的。

例子：

在线性回归中，我们的损失是：
$$
L(\mathbf{w},b)=\dfrac{1}{n}\sum_{i=1}^n\dfrac{1}{2}\Big(\mathbf{w}^\top\mathbf{x}^{(i)}+b-y^{(i)}\Big)^2
$$
我们通过*正则化常数*`λ`来平衡这个新的额外惩罚， 这是一个非负超参数，我们使用验证数据拟合：
$$
L(\mathbf{w},b)+\dfrac{\lambda}{2}\|\mathbf{w}\|^2
$$
L2正则化回归的小批量随机梯度下降更新如下式：
$$
\text{w}\leftarrow(1-\eta\lambda)\text{w}-\dfrac{\eta}{|\mathcal{B}|}\sum\limits_{i\in\mathcal{B}}\mathbb{x}^{(i)}\left(\text{w}^\top\mathbf{x}^{(i)}+b-y^{(i)}\right)
$$
我们根据估计值与观测值之间的差异来更新w。 然而，我们同时也在试图将w的大小缩小到零。 这就是为什么这种方法有时被称为*权重衰减*。 我们仅考虑惩罚项，优化算法在训练的每一步*衰减*权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的λ值对应较少约束的w， 而较大的λ值对w的约束更大。

是否对相应的偏置b^2^进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项不会被正则化。

## 暂退法 - Dropout

泛化性和灵活性之间的这种基本权衡被描述为`偏差-方差权衡（bias-variance tradeoff）`。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。

经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。

- 简单性以较小维度的形式展现， 我们在 [4.4节](https://zh.d2l.ai/chapter_multilayer-perceptrons/underfit-overfit.html#sec-model-selection) 讨论线性模型的单项式函数时探讨了这一点。 此外，正如我们在[4.5节](https://zh.d2l.ai/chapter_multilayer-perceptrons/weight-decay.html#sec-weight-decay) 中讨论权重衰减（L2正则化）时看到的那样， 参数的范数也代表了一种有用的简单性度量。

- 简单性的另一个角度是平滑性，即函数不应该对其输入的微小变化敏感。

  > 有人提出了一个想法： 在训练过程中，他们建议在计算后续层之前向网络的每一层注入噪声。 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性。

  这个想法被称为*暂退法*（dropout）。 暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 

  > 暂退法的原始论文提到了一个关于有性繁殖的类比： 神经网络过拟合与每一层都依赖于前一层激活值相关，称这种情况为“共适应性”。 作者认为，暂退法会破坏共适应性，就像有性生殖会破坏共适应的基因一样。

如何注入噪声：一种想法是以一种*无偏向*（unbiased）的方式注入噪声。 这样在固定住其他层时，***每一层的期望值等于没有噪音时的值***。

在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值`h`以*暂退概率*`p`由随机变量`h′`替换，如下所示：
$$
h=\left\{\begin{matrix}0&P\\ \frac{h}{1-p}&\mathrm{other}\end{matrix}\right.
$$
根据此模型的设计，其期望值保持不变，即$E[h'] = h $

**通常，我们在测试时不用暂退法。** 给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化。 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性”： 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。

> PyTorch 中`nn.Dropout`默认在`train()`时生效，`eval()`不生效，若想在验证时生效，可手动开启：
> ```python
> net.eval()
> def enable_dropout(m):
>   if isinstance(m, nn.Dropout):
>     m.train()		# 手动开启 Dropout 的训练模式，就可以生效了
> net.apply(enable_dropout)
> 
> y = net(X)	# 正常运行即可
> ```

> 除了标准暂退法技术的方法，我们可以考虑一些其他的正则化方法，比如`噪声注入（Noise Injection）`。
>
> 噪声注入是一种在深度学习中应用广泛的正则化方法，**可以通过向输入数据、权重或激活值中注入随机噪声**来增加模型的鲁棒性，从而提高泛化能力。
