---
layout: doc
title: 预备知识
createTime: 2023/3/17
preview: 要学习深度学习，首先需要先掌握一些基本技能。
---

# 预备知识

> 包含必备的 Python 操作、线性代数以及简单的数学知识

## 数据操作

### 节约内存

> 数据原地操作

``使用切片表示法将操作的结果分配给先前分配的数组``

执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。 为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同， 使用`zeros_like`来分配一个全0的块。

```python
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))
#
id(Z): 139931132035296
id(Z): 139931132035296
```

如果在后续计算中没有重复使用`X`， 我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。

```python
before = id(X)
X += Y
id(X) == before
#
True
```

## 线性代数

### 张量算法的基本性质

#### Hadamard product ⊙

两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号⊙）。

![image-20230312111525665](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312111525665.png)

#### 非降维运算（重点在非降维）

``keepdims``参数与``广播机制``

```python
# 求和
sum_A = A.sum(axis=1, keepdims=True)
sum_A
#
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
# 归一化
A / sum_A
#
tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```

#### 点积（Dot Product）

``torch.dot()``

两向量各元素相乘再相加。

![image-20230312112320864](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112320864.png)

#### 矩阵-向量 积（矩阵 * 向量）

``torch.mv()``

矩阵各行向量与向量的点积：

![image-20230312112448685](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112448685.png)

```python
A.shape, x.shape, torch.mv(A, x)
# 
(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```

#### 矩阵相乘

> 仅作为与``Hadamard 积``有区别的提醒

![image-20230312112803028](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112803028.png)

#### 范数

> 线性代数中最有用的一些运算符是*范数*（``norm``）。 非正式地说，向量的*范数*是表示一个向量有多大。 这里考虑的*大小*（``size``）概念不涉及维度，而是分量的大小。

![image-20230312113045447](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113045447.png)

##### L2范数

> *L2范数*是向量元素平方和的平方根

![image-20230312113144939](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113144939.png)

```python
# torch.norm()
u = torch.tensor([3.0, -4.0])
torch.norm(u)
# tensor(5.)
```

##### L1范数

> *L1范数*是向量元素的绝对值之和

![image-20230312113346063](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113346063.png)

```python
torch.abs(u).sum()
# tensor(7.)
```

##### *Frobenius范数*（针对矩阵）

> 是矩阵元素平方和的平方根

![image-20230312113539564](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113539564.png)

```python
# torch.norm()
torch.norm(torch.ones((4, 9)))
# tensor(6.)
```

## 微积分

### 梯度（gradient）

> 就是函数的偏导；对于向量输入而言，偏导的输出也是一个向量：就是偏导向量，称为梯度向量。

![image-20230312114226705](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312114226705.png)

## 自动微分

> 实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

### 非标量变量的反向传播

在``PyTorch``中有个简单的规定，不让张量对张量求导，只允许标量对张量求导。因此，目标量对一个非标量调用``backward()``，则需要传入一个``gradient参数``。传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。

```python
x = torch.arange(4.0, requires_grad=True)
y = x * x  # y 是向量
# y.backward()报错
# 例：求偏导数的和，那么给每个分量的梯度是1
y.backward(gradient=torch.ones(len(x)))
# gradient 参数是对张量 y 进行 乘法运算，它就能输出一个标量了
# 记 gradient 为张量 w，点积是向量维度的
# 若 y (亦即w) 是更高维度的张量，看作是 y 与 w 在每个维度上做点积，这样便输出一个标量了
# 这就是将 对张量的反向传播 => 对标量的反向传播
# 该例中等价于: y.sum().backward()

# 例：求偏导的平均
y.backward(torch.ones_like(x)/x.numel())
# === y.mean().backward()

# gradient 参数表征的是 分配不同的权重 给各分量。因为不用的值的梯度对函数结果的影响程度可能不同。
```

### 分离计算

``[tensor].detach()``

将某些计算移动到记录的计算图之外。

这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 **换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。**

```python
x.grad.zero_()
y = x * x
u = y.detach()  # 分离操作
z = u * x

z.sum().backward()
x.grad == u
# tensor([True, True, True, True])

# 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
# tensor([True, True, True, True])
```

## 概率

### 概率论公理

![image-20230324125352715](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324125352715.png)

### 处理多个随机变量

#### 联合概率

![image-20230324130537817](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130537817.png)

#### 条件概率

![image-20230324130621959](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130621959.png)

#### 贝叶斯定理

![image-20230324130755791](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324130755791.png)

#### 边际化

![image-20230324131036973](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324131036973.png)

#### 独立性

![image-20230324131429633](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324131429633.png)

#### 期望与方差

![image-20230324133554377](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324133554377.png)

