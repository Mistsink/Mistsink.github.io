---
layout: doc
title: 线性神经网络
createTime: 2023/3/24
preview: 我们从经典算法-线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。
---

# 线性神经网络

## 线性回归

> *回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。

### 基本元素

#### 线性模型

![image-20230324145628037](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324145628037.png)

> 即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。
>
> 在开始寻找最好的*模型参数*（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

#### 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。

为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）。

![image-20230324150109654](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324150109654.png)

#### 解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）

> 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

#### 随机梯度下降

***梯度下降*（gradient descent）**这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

#### 单层神经网络

![image-20230324192345662](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324192345662.png)

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说， [图3.1.2]中神经网络的*层数*为1。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（ [图3.1.2]中的输出层） 称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。