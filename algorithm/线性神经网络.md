---
layout: doc
title: 线性神经网络
createTime: 2023/3/24
preview: 我们从经典算法-线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。
---

# 线性神经网络

## 线性回归

> *回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。

### 基本元素

#### 线性模型

![image-20230324145628037](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324145628037.png)

> 即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。
>
> 在开始寻找最好的*模型参数*（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

#### 损失函数

*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。

为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）。

![image-20230324150109654](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324150109654.png)

#### 解析解

线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）

> 解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。

#### 随机梯度下降

***梯度下降*（gradient descent）**这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。

#### 单层神经网络

![image-20230324192345662](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324192345662.png)

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说， [图3.1.2]中神经网络的*层数*为1。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（ [图3.1.2]中的输出层） 称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。

## ***Softmax***回归

> 通常，机器学习实践者用*分类*这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

- 独热编码

### 网络架构

为了解决线性模型的分类问题，我们需要和输出一样多的*仿射函数*（affine function）。 每个输出对应于它自己的仿射函数。

![image-20230325235115225](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230325235115225.png)

### Softmax 运算

然而我们能否将未规范化的预测o直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 这些违反了 [2.6节](https://zh.d2l.ai/chapter_preliminaries/probability.html#sec-prob)中所说的概率基本公理。

**要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 这个属性叫做*校准*（calibration）。**

***softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。***

我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。

![image-20230325235946262](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230325235946262.png)

> 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型*（linear model）。

### 损失函数

> 最大似然估计

![image-20230326001543382](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326001543382.png)

### Softmax及其导数

![image-20230326002527550](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326002527550.png)

> 换句话说，**导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值y和估计值y^之间的差异**。 这不是巧合，在任何指数族分布模型中 （参见[本书附录中关于数学分布的一节](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html)）， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。

### 交叉熵损失

> 我们将通过介绍信息论基础来理解交叉熵损失。 如果想了解更多信息论的细节，请进一步参考 [本书附录中关于信息论的一节](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)。

#### 信息论

> *信息论*（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布P的*熵*（entropy）。可以通过以下方程得到：

![image-20230326002917389](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326002917389.png)

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到”惊异”。 克劳德·香农决定用信息量

![image-20230326003434645](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326003434645.png)

来量化这种惊异程度。 在观察一个事件j时，并赋予它（主观）概率P(j)。 **当我们赋予一个事件较低的概率时，我们的惊异会更大（就是上面那个表达式的值越大），该事件的信息量也就更大（惊异越大就说明我们对事件了解程度很少，相对来说就是事件包含的信息很多）。**

 ***所以理解：在 [(3.4.11)](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#equation-eq-softmax-reg-entropy)中定义的熵， 是当分配的概率真正匹配数据生成过程时的信息量的期望。***

#### 重新认识交叉熵

如果把熵H(P)想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？ ***交叉熵从`P`到`Q`，记为`H(P,Q)`。 我们可以把交叉熵想象为“主观概率为`Q`的观察者在看到根据概率`P`生成的数据时的预期惊异”。 当`Q=P`时，交叉熵达到最低。 在这种情况下，从`Q`到`P`的交叉熵是`H(P,P)=H(P)`。***

> 简而言之，我们可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。

### 实践

在PyTorch中，通常将softmax和交叉熵损失函数（CrossEntropyLoss）结合在一起使用，是因为这样可以提高计算效率。***具体而言，CrossEntropyLoss函数在内部已经实现了softmax操作和对数计算，因此无需显式地调用softmax函数。***直接将模型的输出值送入CrossEntropyLoss函数中即可计算softmax及其对数，并且可以避免数值稳定性的问题。

需要注意的是，如果使用了其他的损失函数，或者需要在模型的输出之后进行其他的操作（例如计算预测精度），则可能需要显式地调用softmax函数。