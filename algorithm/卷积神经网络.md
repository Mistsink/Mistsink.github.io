---
layout: doc
title: 卷积神经网络
createTime: 2023/3/29
preview: 卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 
---

# 卷积神经网络

> 卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。

## 从全连接层到卷积

### 不变性

假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。

卷积神经网络正是将*空间不变性*（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。

1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

### 多层感知机的限制

多层感知机的输入是二维图像$$X$$，其隐藏表示$$H$$在数学上是一个矩阵，在代码中表示为二维张量。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$$W$$。假设$$U$$包含偏置参数，我们可以将全连接层形式化地表示为
$$
\begin{aligned}[\mathbf{H}]_{i,j}&=[\mathbf{U}]_{i,j}+\sum_{k}\sum_{l}[\mathbf{W}]_{i,j,k,l}[\mathbf{X}]_{k,l}\\ &=[\mathbf{U}{]_{i,j}}+\sum_{a}\sum_{b}[\mathbf{V}]_{i,j,a,b}[\mathbf{X}{]_{i+a,j+b}}\end{aligned}
$$
索引$$a$$和$$b$$通过在正偏移和负偏移之间移动覆盖了整个图像。

平移不变性。 这意味着检测对象在输入$$X$$中的平移，应该仅导致隐藏表示$$H$$中的平移。也就是说，$$V$$和$$U$$实际上不依赖于$$(i, j)$$的值，即$$[\mathsf{V}]_{i,j,a,b}=[\mathbf{V}]_{a,b}$$。
$$
[\mathbf{H}]_{i,j}=u+\sum_a\sum_b[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}
$$
这就是`卷积（convolution）`。我们是在使用系数$$[\mathbf{V}]_{a,b}$$对位置$$(i, j)$$附近的像素$(i+a, j+ b)$进行加权得到$[\mathbf{H}]_{i,j}$。 注意，$$[\mathbf{V}]_{a,b}$$的系数比$[\mathsf{V}]_{i,j,a,b}$少很多，因为前者不再依赖于图像中的位置。这就是显著的进步！

局部性。这意味着在$|a|>\Delta$或$|b|>\Delta$的范围之外，我们可以设置$[\mathbf{V}]_{a,b}=0$。
$$
[\mathbf H]_{i,j}=\mathbf u+\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta[\mathbf V]_{a,b}[\mathbf X]_{i+a,j+b}
$$

> 这便是一个卷积层，而卷积神经网络是包含卷积层的一类特殊的神经网络。 在深度学习研究社区中，$\mathbf{V}$被称为*卷积核*（convolution kernel）或者*滤波器*（filter），亦或简单地称之为该卷积层的*权重*，通常该权重是可学习的参数。

### 卷积

![image-20230330113438063](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230330113438063.png)

### 通道

对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的*通道*（channel）。 这些通道有时也被称为*特征映射*（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。 

> 说白了就是原本单一的全局的权重学习，增加细化成了多个权重学习器，所以多出了一个维度$d$来存储这种权重：原本的$(a * b * c)$可以视为$(a *b*c*1)$，是一个全局的权重转换，现在成了$(a * b*c*d)$就是把最后的$(1)$改成了$(d)$，可以存储一个权重空间，而不仅仅是一个权重标量

## 图像卷积

> 在图像数据中的应用

### 互相关运算

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是*互相关运算*（cross-correlation）。

![image-20230330140342226](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230330140342226.png)

### 卷积层

高度和宽度分别为$h$和$w$的卷积核可以被称为$h \times w$卷积或$h \times w$卷积核。 我们也将带有$h \times w$卷积核的卷积层称为$h \times w$卷积层。







