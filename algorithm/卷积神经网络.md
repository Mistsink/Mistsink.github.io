---
layout: doc
title: 卷积神经网络
createTime: 2023/3/29
preview: 卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。 
---

# 卷积神经网络

> 卷积神经网络（convolutional neural network，CNN）是一类强大的、为处理图像数据而设计的神经网络。

## 从全连接层到卷积

### 不变性

假设我们想从一张图片中找到某个物体。 合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。

卷积神经网络正是将*空间不变性*（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。

现在，我们将上述想法总结一下，从而帮助我们设计适合于计算机视觉的神经网络架构。

1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

### 多层感知机的限制

多层感知机的输入是二维图像$$X$$，其隐藏表示$$H$$在数学上是一个矩阵，在代码中表示为二维张量。为了使每个隐藏神经元都能接收到每个输入像素的信息，我们将参数从权重矩阵（如同我们先前在多层感知机中所做的那样）替换为四阶权重张量$$W$$。假设$$U$$包含偏置参数，我们可以将全连接层形式化地表示为
$$
\begin{aligned}[\mathbf{H}]_{i,j}&=[\mathbf{U}]_{i,j}+\sum_{k}\sum_{l}[\mathbf{W}]_{i,j,k,l}[\mathbf{X}]_{k,l}\\ &=[\mathbf{U}{]_{i,j}}+\sum_{a}\sum_{b}[\mathbf{V}]_{i,j,a,b}[\mathbf{X}{]_{i+a,j+b}}\end{aligned}
$$
索引$$a$$和$$b$$通过在正偏移和负偏移之间移动覆盖了整个图像。

平移不变性。 这意味着检测对象在输入$$X$$中的平移，应该仅导致隐藏表示$$H$$中的平移。也就是说，$$V$$和$$U$$实际上不依赖于$$(i, j)$$的值，即$$[\mathsf{V}]_{i,j,a,b}=[\mathbf{V}]_{a,b}$$。
$$
[\mathbf{H}]_{i,j}=u+\sum_a\sum_b[\mathbf{V}]_{a,b}[\mathbf{X}]_{i+a,j+b}
$$
这就是`卷积（convolution）`。我们是在使用系数$$[\mathbf{V}]_{a,b}$$对位置$$(i, j)$$附近的像素$(i+a, j+ b)$进行加权得到$[\mathbf{H}]_{i,j}$。 注意，$$[\mathbf{V}]_{a,b}$$的系数比$[\mathsf{V}]_{i,j,a,b}$少很多，因为前者不再依赖于图像中的位置。这就是显著的进步！

局部性。这意味着在$|a|>\Delta$或$|b|>\Delta$的范围之外，我们可以设置$[\mathbf{V}]_{a,b}=0$。
$$
[\mathbf H]_{i,j}=\mathbf u+\sum_{a=-\Delta}^\Delta\sum_{b=-\Delta}^\Delta[\mathbf V]_{a,b}[\mathbf X]_{i+a,j+b}
$$

> 这便是一个卷积层，而卷积神经网络是包含卷积层的一类特殊的神经网络。 在深度学习研究社区中，$\mathbf{V}$被称为*卷积核*（convolution kernel）或者*滤波器*（filter），亦或简单地称之为该卷积层的*权重*，通常该权重是可学习的参数。

### 卷积

![image-20230330113438063](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230330113438063.png)

### 通道

对于每一个空间位置，我们想要采用一组而不是一个隐藏表示。这样一组隐藏表示可以想象成一些互相堆叠的二维网格。 因此，我们可以把隐藏表示想象为一系列具有二维张量的*通道*（channel）。 这些通道有时也被称为*特征映射*（feature maps），因为每个通道都向后续层提供一组空间化的学习特征。 

> 说白了就是原本单一的全局的权重学习，增加细化成了多个权重学习器，所以多出了一个维度$d$来存储这种权重：原本的$(a * b * c)$可以视为$(a *b*c*1)$，是一个全局的权重转换，现在成了$(a * b*c*d)$就是把最后的$(1)$改成了$(d)$，可以存储一个权重空间，而不仅仅是一个权重标量

## 图像卷积

> 在图像数据中的应用

### 互相关运算

严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是*互相关运算*（cross-correlation）。

![image-20230330140342226](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230330140342226.png)

### 卷积层

高度和宽度分别为$h$和$w$的卷积核可以被称为$h \times w$卷积或$h \times w$卷积核。 我们也将带有$h \times w$卷积核的卷积层称为$h \times w$卷积层。

### 互相关和卷积

为了得到正式的*卷积*运算输出，我们需要执行 [(6.1.6)](https://zh.d2l.ai/chapter_convolutional-neural-networks/why-conv.html#equation-eq-2d-conv-discrete)中定义的严格卷积运算，而不是互相关运算。 幸运的是，它们差别不大，我们只需水平和垂直翻转二维卷积核张量，然后对输入张量执行*互相关*运算。

由于卷积核是从数据中学习到的，因此无论这些层执行严格的卷积运算还是互相关运算，卷积层的输出都不会受到影响。 

### 感受野 `receptive field`

 在卷积神经网络中，对于某一层的任意元素$x$，其*感受野*（receptive field）是指在前向传播期间可能影响$x$计算的所有元素（来自所有先前层）。

## 填充和步幅

假设输入形状为$n_n\times n_w$，卷积核形状为$k_n\times k_w$，那么输出形状将是$(n_h-k_h + 1) \times (n_w - k_w + 1)$。

如此一来:

- 原始图像的边界丢失了许多有用信息。而`填充`是解决此问题最有效的方法；
- 有时，我们可能希望大幅降低图像的宽度和高度。例如，如果我们发现原始的输入分辨率十分冗余。`步幅`则可以在这类情况下提供帮助。

通常，如果我们添加$p_h$行填充（大约一半在顶部，一半在底部）和$p_w$列填充（左侧大约一半，右侧一半），则输出形状将为
$$
(n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)
$$
在许多情况下，我们需要设置$p_h = k_h - 1$和$p_w = k_w -1$，使输入和输出具有相同的高度和宽度。 这样可以在构建网络时更容易地预测每个图层的输出形状。

> 卷积神经网络中卷积核的高度和宽度通常为奇数，例如1、3、5或7。 选择奇数的好处是，保持空间维度的同时，我们可以在顶部和底部填充相同数量的行，在左侧和右侧填充相同数量的列。

```python
# eg:
# 为了方便起见，我们定义了一个计算卷积层的函数。
# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数
def comp_conv2d(conv2d, X):
    # 这里的（1，1）表示批量大小和通道数都是1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # 省略前两个维度：批量大小和通道
    return Y.reshape(Y.shape[2:])

# 请注意，这里每边都填充了1行或1列，因此总共添加了2行或2列
conv2d = nn.Conv2d(1, 1, kernel_size=(5, 3), padding=(2, 1))
X = torch.rand(size=(8, 8))
comp_conv2d(conv2d, X).shape	# torch.Size([8, 8])
```

我们将每次滑动元素的数量称为*步幅*（stride）。

通常，当垂直步幅为$s_h$、水平步幅为$s_w$时，输出形状为
$$
\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor\times\lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor
$$

```python
# eg:
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape		# torch.Size([2, 2])
```

## 多输入多输出通道

### 1 x 1 卷积层

$1 \times 1$卷积，即$k_h=k_w=1$，看起来似乎没有多大意义。 毕竟，卷积的本质是有效提取相邻像素间的相关特征，而$1 \times 1$卷积显然没有此作用。 尽管如此，$1 \times 1$仍然十分流行，经常包含在复杂深层网络的设计中。

$1 \times 1$卷积的唯一计算发生在通道上。

![image-20230401190636686](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230401190636686.png)

- 当以每像素为基础应用时，$1 \times 1$卷积层相当于全连接层。
- $1 \times 1$卷积层通常用于调整网络层的通道数量和控制模型复杂性。

## 池化层

*池化*（pooling）层，它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。

### 最大池化、平均池化

池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为*最大池化层*（maximum pooling）和*平均池化层*（average pooling）。

默认情况下，深度学习框架中的步幅与汇聚窗口的大小相同。 

## 卷积神经网络 LeNet

![image-20230401204745745](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230401204745745.png)

每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。

> 请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。

![image-20230401205450134](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230401205450134.png)

- 卷积神经网络（CNN）是一类使用卷积层的网络。
- 在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。
- 为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。
- 在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。
- LeNet是最早发布的卷积神经网络之一。
