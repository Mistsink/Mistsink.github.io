---
layout: doc
title: 现代卷积神经网络
createTime: 2023/4/2
preview: 将介绍现代的卷积神经网络架构。 AlexNet、使用重复块的网络（VGG）、网络中的网络（NiN）、含并行连结的网络（GoogLeNet）、残差网络（ResNet）、稠密连接网络（DenseNet）
---

# 现代卷积神经网络

- `AlexNet`。它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；
- `使用重复块的网络（VGG）`。它利用许多重复的神经网络块；
- `网络中的网络（NiN）`。它重复使用由卷积层和卷积层（用来代替全连接层）来构建深层网络;
- `含并行连结的网络（GoogLeNet）`。它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；
- `残差网络（ResNet）`。它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；
- `稠密连接网络（DenseNet）`。它的计算成本很高，但给我们带来了更好的效果。

## 深度卷积神经网络 AlexNet

> 2012年，AlexNet横空出世。它首次证明了学习到的特征可以超越手工设计的特征。它一举打破了计算机视觉研究的现状。 

![image-20230402134346211](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402134346211.png)

AlexNet和LeNet的设计理念非常相似，但也存在显著差异。

1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。
2. AlexNet使用ReLU而不是sigmoid作为其激活函数。

容量控制和预处理：

AlexNet通过暂退法（ [4.6节](https://zh.d2l.ai/chapter_multilayer-perceptrons/dropout.html#sec-dropout)）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。 为了进一步扩充数据，AlexNet在训练时增加了大量的图像增强数据，如翻转、裁切和变色。 这使得模型更健壮，更大的样本量有效地减少了过拟合。

## 使用块的网络 VGG

虽然AlexNet证明深层神经网络卓有成效，但它没有提供一个通用的模板来指导后续的研究人员设计新的网络。 在下面的几个章节中，我们将介绍一些常用于设计深层神经网络的启发式概念。

### VGG 块

经典卷积神经网络的基本组成部分是下面的这个序列：

1. 带填充以保持分辨率的卷积层；
2. 非线性激活函数，如ReLU；
3. 汇聚层，如最大汇聚层。

![image-20230402141304184](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402141304184.png)

- VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
- 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
- 在VGG论文中，Simonyan和Ziserman尝试了各种架构。***特别是他们发现深层且窄的卷积（即）比较浅层且宽的卷积更有效。***

## 网络中的网络 NiN

LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。

*网络中的网络*（*NiN*）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机 ([Lin *et al.*, 2013](https://zh.d2l.ai/chapter_references/zreferences.html#id93))

### NiN 块

***NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。*** 

如果我们将权重连接到每个空间位置，我们可以将其视为$1 \times 1$卷积层（如 [6.4节](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html#sec-channels)中所述），或作为在每个像素位置上独立作用的全连接层。 从另一个角度看，即将空间维度中的每个像素视为单个样本，将通道维度视为不同特征（feature）。

![image-20230402145342150](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402145342150.png)

***NiN和AlexNet之间的一个显著区别是NiN完全取消了全连接层。*** 相反，NiN使用一个NiN块，其输出通道数等于标签类别的数量。最后放一个*全局平均汇聚层*（global average pooling layer），生成一个对数几率 （logits）。

NiN设计的一个优点是，它显著减少了模型所需参数的数量。然而，在实践中，这种设计有时会增加训练模型的时间。

- NiN使用由一个$1 \times 1$卷积层和多个卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
- NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。
- 移除全连接层可减少过拟合，同时显著减少NiN的参数。

## 含并行连结到网络 GoogLeNet

这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。 本文的一个观点是，有时使用不同大小的卷积核组合是有利的。 

### Inception 块

在GoogLeNet中，基本的卷积块被称为*Inception块*（Inception block）。

![image-20230402154232010](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402154232010.png)

Inception块由四条并行路径组成。 前三条路径使用窗口大小为$1 \times 1$、$3 \times 3$和$5 \times 5$的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行$1 \times 1$卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用$3 \times 3$最大汇聚层，然后使用$1 \times 1$卷积层来改变通道数。

 这四条路径都使用合适的填充来使输入与输出的高和宽一致，最后我们将每条线路的输出在通道维度上连结，并构成Inception块的输出。在Inception块中，通常调整的超参数是每层输出通道数。

```python
class Inception(nn.Module):
    # c1--c4是每条路径的输出通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # 线路1，单1x1卷积层
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # 线路2，1x1卷积层后接3x3卷积层
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1x1卷积层后接5x5卷积层
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3x3最大汇聚层后接1x1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # 在通道维度上连结输出
        return torch.cat((p1, p2, p3, p4), dim=1)
```

首先我们考虑一下滤波器（filter）的组合，它们可以用各种滤波器尺寸探索图像，这意味着不同大小的滤波器可以有效地识别不同范围的图像细节。 同时，我们可以为不同的滤波器分配不同数量的参数。

### GoogLeNet 模型

GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。

第一个模块类似于AlexNet和LeNet，Inception块的组合从VGG继承，全局平均汇聚层避免了在最后使用全连接层。

![image-20230402155311255](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402155311255.png)

## 批量规范化

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移。

> 请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。

![image-20230402202055515](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402202055515.png)

![image-20230402202857275](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402202857275.png)

> 批量规范化最适应范围$50 \sim 100$中的中等批量大小。

另外，批量规范化层在”训练模式“（通过小批量统计数据规范化）和“预测模式”（通过数据集统计规范化）中的功能不同。 

- 在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。 
- 而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差。

### 批量规范化层

#### 全连接层

通常，我们将批量规范化层置于全连接层中的仿射变换和激活函数之间。 

#### 卷积层

对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。 

当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量规范化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。平均和标准差是每个通道各自独立的均值和标准差。

#### 预测时

- 首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。 
- 其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。 一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。

```python
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data
```

## 残差网络 ResNet

随着我们设计越来越深的网络，深刻理解“新添加的层如何提升神经网络的性能”变得至关重要。更重要的是设计网络的能力，在这种网络中，添加层会使网络更具表现力。

### 函数类

假设有一类特定的神经网络架构$\mathcal{F}$，它包括学习速率和其他超参数设置。 对于所有$f\in\mathcal{F}$，存在一些参数集（例如权重和偏置），这些参数可以通过在合适的数据集上进行训练而获得。 现在假设$f^*$是我们真正想要找到的函数，如果是$f^*\in\mathcal{F}$，那我们可以轻而易举的训练得到它，但通常我们不会那么幸运。 相反，我们将尝试找到一个函数$f^*_\mathcal{F}$，这是我们在$\mathcal{F}$中的最佳选择。
$$
f_F^*:=\underset{f}{\operatorname{argmin}}L(\mathbf{X},\mathbf{y},f)\text{subject to}f\in\mathcal{F}
$$
![image-20230402215128649](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402215128649.png)

因此，只有当较复杂的函数类包含较小的函数类时，我们才能确保提高它们的性能。

`残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。`

### 残差块

让我们聚焦于神经网络局部：如图 [图7.6.2](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#fig-residual-block)所示，假设我们的原始输入为$x$，而希望学出的理想映射为$f(x)$（作为 [图7.6.2](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#fig-residual-block)上方激活函数的输入）。 [图7.6.2](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#fig-residual-block)左图虚线框中的部分需要直接拟合出该映射$f(x)$，而右图虚线框中的部分则需要拟合出残差映射$f(x) - x$。 残差映射在现实中往往更容易优化。

 以本节开头提到的恒等映射作为我们希望学出的理想映射$f(x)$，我们只需将 [图7.6.2](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#fig-residual-block)中右图虚线框内上方的加权运算（如仿射）的权重和偏置参数设成0，那么$f(x)$即为恒等映射。 实际中，当理想映射$f(x)$极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 [图7.6.2](https://zh.d2l.ai/chapter_convolutional-modern/resnet.html#fig-residual-block)右图是ResNet的基础架构–*残差块*（residual block）。

![image-20230402221917879](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230402221917879.png)

> “残差”指的是输入和输出的差异值，而跨层连接是直接将输入特征图通过跨越一些层，与输出特征图相加。

```python
class Residual(nn.Module):  #@save
    def __init__(self, input_channels, num_channels,
                 use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels,
                               kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels,
                               kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
```

- 学习嵌套函数（nested function）是训练神经网络的理想情况。在深层神经网络中，学习另一层作为恒等映射（identity function）较容易（尽管这是一个极端情况）。
- 残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。
- 利用残差块（residual blocks）可以训练出一个有效的深层神经网络：输入可以通过层间的残余连接更快地向前传播。
- 残差网络（ResNet）对随后的深层神经网络设计产生了深远影响。

## 稠密连接网络 DenseNet

### 从 ResNet 到 DenseNet

任意函数的泰勒展开式（Taylor expansion），它把这个函数分解成越来越高阶的项。在接近0时：
$$
f(x)=f(0)+f'(0)x+\dfrac{f''(0)}{2!}x^2+\dfrac{f'''(0)}{3!}x^3+\cdots\cdots
$$
同样，ResNet 将函数展开为：
$$
f(\mathbf{x})=\mathbf{x}+g(\mathbf{x})
$$
也就是说，ResNet将$f$分解为两部分：一个简单的线性项和一个复杂的非线性项。 那么再向前拓展一步，如果我们想将$f$拓展成超过两部分的信息呢？ 一种方案便是`DenseNet`。

![image-20230403141226139](/Users/firework/Library/Application Support/typora-user-images/image-20230403141226139.png)

ResNet和DenseNet的关键区别在于，DenseNet输出是*连接*（用图中的表示）而不是如ResNet的简单相加。 

![image-20230403141927141](/Users/firework/Library/Application Support/typora-user-images/image-20230403141927141.png)

稠密网络主要由2部分构成：*稠密块*（dense block）和*过渡层*（transition layer）。 前者定义如何连接输入和输出，而后者则控制通道数量，使其不会太复杂。

### 稠密块体

DenseNet使用了ResNet改良版的“批量规范化、激活和卷积”架构。

```python
def conv_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))
    
# 一个稠密块由多个卷积块组成，每个卷积块使用相同数量的输出通道。
class DenseBlock(nn.Module):
    def __init__(self, num_convs, input_channels, num_channels):
        super(DenseBlock, self).__init__()
        layer = []
        for i in range(num_convs):
            layer.append(conv_block(
                num_channels * i + input_channels, num_channels))
        self.net = nn.Sequential(*layer)

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            # 连接通道维度上每个块的输入和输出
            X = torch.cat((X, Y), dim=1)
        return X
```

卷积块的通道数控制了输出通道数相对于输入通道数的增长，因此也被称为*增长率*（growth rate）。

### 过渡层

它通过卷积层来减小通道数，并使用步幅为2的平均汇聚层减半高和宽，从而进一步降低模型复杂度。

```python
def transition_block(input_channels, num_channels):
    return nn.Sequential(
        nn.BatchNorm2d(input_channels), nn.ReLU(),
        nn.Conv2d(input_channels, num_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
```

