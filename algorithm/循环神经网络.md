---
layout: doc
title: 循环神经网络
createTime: 2023/4/3
preview: 要学习深度学习，首先需要先掌握一些基本技能。
---

# 循环神经网络

我们遇到过两种类型的数据：表格数据和图像数据。 对于图像数据，我们设计了专门的卷积神经网络架构来为这类特殊的数据结构建模。

最重要的是，到目前为止我们默认数据都来自于某种分布， 并且所有样本都是独立同分布的 （independently and identically distributed，i.i.d.）。

***简言之，如果说卷积神经网络可以有效地处理空间信息， 那么本章的*循环神经网络*（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。***

## 序列模型

在一个序列 $x_1,x_2,x_3 ······ x_t$中，$x_t$表示时间$t$时的结果，$t$称为时间步(time step) $t \in\mathbb{Z}^+$，于是通过以下途径预测$x_t$：
$$
x_t\sim P(x_t\mid x_{t-1},\ldots,x_1)
$$

### 自回归模型 （autoregressive models）

简单来说，归结为两种策略：

- 假设在现实情况下相当长的序列 $x_{t-1},...,x_1$ 可能是不必要的， 因此我们***只需要满足某个长度为$\tau$的时间跨度， 即使用观测序列$x_{t-1},\ldots,x_{t-\tau}$。*** 当下获得的最直接的好处就是参数的数量总是不变的， 至少在$t > \tau$时如此，这就使我们能够训练一个上面提及的深度网络。
- 保留一些对过去观测的总结$h_t$， 并且同时更新预测$\hat{x}_t$和总结$h_t$。 这就产生了基于$\hat{x}_t=P(x_t\mid h_t)$估计$x_t$， 以及公式$h_t=g(h_{t-1},x_{t-1})$更新的模型。 
  由于$h_t$从未被观测到，这类模型也被称为 *隐变量自回归模型*（latent autoregressive models）。

![image-20230403170017698](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230403170017698.png)

### 马尔可夫模型

在自回归模型的近似法中， 我们使用$x_{t-1},\ldots,x_{t-\tau}$而不是$x_{t-1},...,x_1$，只要这种是近似精确的，我们就说序列满足*马尔可夫条件*（Markov condition）。

特别是，如果$\tau=1$，得到一个 *一阶马尔可夫模型*（first-order Markov model），$P(x)$ 由下式给出：
$$
P(x_1,\ldots,x_T)=\prod\limits_{t=1}^T P(x_t\mid x_{t-1})\cong P(x_1\mid x_0)=P(x_1)
$$
当假设$x_t$仅是离散值时，这样的模型特别棒， 因为在这种情况下，***使用动态规划可以沿着马尔可夫链精确地计算结果。*** 例如，我们可以高效地计算$P(x_{t+1} | x_{t-1})$：

![image-20230403174915456](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230403174915456.png)

## 文本预处理

一般步骤：

1. 将文本作为字符串加载到内存中。
2. 将字符串拆分为词元（如单词和字符）。
3. 建立一个词表，将拆分的词元映射到数字索引。
4. 将文本转换为数字索引序列，方便模型操作。

## 学习语言模型

### 马尔可夫模型与 n 元语法

如果$P(x_{t+1}|x_t,\ldots,x_1)=P(x_{t+1}\mid x_t)$，则序列上的分布满足一阶马尔可夫性质。阶数越高，对应的依赖关系就越长。 这种性质推导出了许多可以应用于序列建模的近似公式：
$$
\begin{array}{ll}&P(x_{1},x_{2},x_{3},x_{4})=P(x_{1})P(x_{2})P(x_{3})P(x_{4}),\\ &P(x_{1},x_2,x_{3},x_4)=P(x_1)P(x_{2}\mid x_{1})P(x_{3}\mid x_{2})P(x_{4}\mid x_{3}),\\ &P(x_1,x_{2},x_{5},x_{4})=P({x_{1}})P({x_{2}}\mid x_{1})P({x_{3}}\mid x_{1},x_{2})P({x_{4}}\mid x_{2},x_{3}).\end{array}
$$
通常，涉及一个、两个和三个变量的概率公式分别被称为 *一元语法*（unigram）、*二元语法*（bigram）和*三元语法*（trigram）模型。 

### 自然语言统计

![image-20230404135140305](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230404135140305.png)

通过此图我们可以发现：词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足*齐普夫定律*（Zipf’s law）， 即第$i$个最常用单词的频率$n_i$为：
$$
n_i\propto\dfrac{1}{i^\alpha}
$$
等价于:
$$
\log n_i=-\alpha\log i+c
$$
当尝试统计多元语法的频率统计结果时：

![image-20230404135556106](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230404135556106.png)

这张图非常令人振奋！原因有很多：

1. 除了一元语法词，单词序列似乎也遵循齐普夫定律， 尽管公式 [(8.3.7)](https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#equation-eq-zipf-law)中的指数$\alpha$更小 （指数的大小受序列长度的影响）；
2. 词表中$n$元组的数量并没有那么大，这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望；
3. 很多$n$元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。

### 读取长序列数据

![image-20230404140727873](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230404140727873.png)

我们可以从随机偏移量开始划分序列， 以同时获得*覆盖性*（coverage）和*随机性*（randomness）。 下面，我们将描述如何实现*随机采样*（random sampling）和 *顺序分区*（sequential partitioning）策略。

#### 随机采样

- 随机数选定最开始的偏移量
- 将切割后的子序列随机打散，使得子序列它们失去原文本顺序

#### 顺序分区

我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

与随机采样的区别在于不会将子序列打散，但同样也有随机偏移量的选定。

- 语言模型是自然语言处理的关键。
- $n$元语法通过截断相关性，为处理长序列提供了一种实用的模型。
- 长序列存在一个问题：它们很少出现或者从不出现。
- 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他$n$元语法。
- 通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
- 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。

## 循环神经网络

隐藏层和隐状态指的是两个截然不同的概念。 如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层， 而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的*输入*， 并且这些状态只能通过先前时间步的数据来计算。

*循环神经网络*（recurrent neural networks，RNNs） 是具有隐状态的神经网络。

### 有隐状态的神经网络

用 $\mathbf{H}_t\in\mathbb{R}^{n\times h}$ 表示时间步的隐藏变量。

具体地说，当前时间步隐藏变量由当前时间步的输入 与前一个时间步的隐藏变量一起计算得出：
$$
\mathbf H_t=\phi(\mathbf X_t\mathbf W_{xh}+\mathbf H_{t-1}\mathbf W_{hh}+\mathbf b_h)
$$
从相邻时间步的隐藏变量$\mathbf{H}_t$和$\mathbf{H}_{t-1}$ 之间的关系可知， 这些变量捕获并保留了序列直到其当前时间步的历史信息， 就如当前时间步下神经网络的状态或记忆， 因此这样的隐藏变量被称为*隐状态*（hidden state）。

由于在当前时间步中， 隐状态使用的定义与前一个时间步中使用的定义相同， 因此 [(8.4.5)](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state)的计算是*循环的*（recurrent）。 于是基于循环计算的隐状态神经网络被命名为 *循环神经网络*（recurrent neural network）。

在循环神经网络中执行 [(8.4.5)](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-rnn-h-with-state)计算的层 称为*循环层*（recurrent layer）。

![image-20230404145753964](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230404145753964.png)

### 基于循环神经网络的字符级语言模型

![image-20230404151651247](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230404151651247.png)

在训练过程中，我们对每个时间步的输出层的输出进行softmax操作， 然后利用交叉熵损失计算模型输出和标签之间的误差。

### 困惑度 Perplexity

让我们讨论如何度量语言模型的质量。在这里，***信息论***可以派上用场了。

***如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。 一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。***

所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：
$$
\dfrac{1}{n}\sum_{t=1}^n-\log P(x_t\mid x_{t-1},\dots,x_1)
$$

> 由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。 简而言之，它是 [(8.4.7)](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn.html#equation-eq-avg-ce-for-lm)的指数：
> $$
> \exp\left(-\dfrac{1}{n}\sum_{t=1}^n\log P(x_t\mid x_{t-1},\dots,x_1)\right)
> $$

## 从零实现RNN

```python
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # 隐藏层参数
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```

`RNN函数定义：在一个time step内计算一个batch`

```python
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    # X的形状：(批量大小，词表大小)
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```

`RNN Moudle`

```python
class RNNModelScratch: #@save
    """从零开始实现的循环神经网络模型"""
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn

    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)

    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
```

试运行：

```python
num_hiddens = 512
net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)
state = net.begin_state(X.shape[0], d2l.try_gpu())
Y, new_state = net(X.to(d2l.try_gpu()), state)
```

预测：

让我们首先定义预测函数来生成`prefix`之后的新字符， 其中的`prefix`是一个用户提供的包含多个字符的字符串。 在循环遍历`prefix`中的开始字符时， ***我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为预热（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测***。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。

```python
def predict_ch8(prefix, num_preds, net, vocab, device):  #@save
    """在prefix后面生成新字符"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]:  # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # 预测num_preds步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```

### 梯度裁剪

对于长度为$T$的序列，我们在迭代中计算这个$T$时间步上的梯度， 将会在反向传播过程中产生长度为$O(T)$的矩阵乘法链。循环神经网络模型往往需要额外的方式来支持稳定训练。

当梯度不稳定时，一个流行的方案是：通过将梯度$g$投影回给定半径 （例如$\theta$）的球来裁剪梯度$g$。
$$
\mathbf{g}\leftarrow\min\left(1,\dfrac{\theta}{\|\mathbf{g}\|}\right)\mathbf{g}
$$
通过这样做，我们知道***梯度范数***永远不会超过$\theta$， 并且更新后的梯度完全与$g$的原始方向对齐。

它还有一个值得拥有的副作用， 即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响， 这赋予了模型一定程度的稳定性。 

> 梯度裁剪提供了一个快速修复梯度爆炸的方法， 虽然它并不能完全解决问题，但它是众多有效的技术之一。

```python
def grad_clipping(net, theta):  #@save
    """裁剪梯度"""
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    # L2 范数
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm	# 更新梯度
```

### 训练

- 当使用顺序分区时， 我们只在每个迭代周期的开始位置初始化隐状态。
  然而，在任何一点隐状态的计算， 都依赖于同一迭代周期中前面所有的小批量数据， 这使得梯度计算变得复杂。 为了降低计算量，在处理任何一个小批量数据之前， 我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。
- 当使用随机抽样时，因为每个样本都是在一个随机位置抽样的， 因此需要为每个迭代周期重新初始化隐状态。

```python
#@save
def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):
    """训练网络一个迭代周期（定义见第8章）"""
    state, timer = None, d2l.Timer()
    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第一次迭代或使用随机抽样时初始化state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # state对于nn.GRU是个张量
                state.detach_()
            else:
                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # 因为已经调用了mean函数
            updater(batch_size=1)
        metric.add(l * y.numel(), y.numel())
    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()
```

## 框架实现

```python
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)
state = torch.zeros((1, batch_size, num_hiddens))
#@save
class RNNModel(nn.Module):
    """循环神经网络模型"""
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)
        # 它的输出形状是(时间步数*批量大小,词表大小)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```

## 通过时间反向传播

 *通过时间反向传播*（backpropagation through time，BPTT） ([Werbos, 1990](https://zh.d2l.ai/chapter_references/zreferences.html#id182))实际上是循环神经网络中反向传播技术的一个特定应用。

它要求我们将循环神经网络的计算图一次展开一个时间步， 以获得模型变量和参数之间的依赖关系。

### 循环神经网络的梯度分析

这个简化模型中，我们将时间步$t$的隐状态表示为$h_t$， 输入表示为$x_t$，输出表示为$o_t$。分别使用$w_n$和$w_o$来表示隐藏层和输出层的权重。每个时间步的隐状态和输出可以写为：
$$
h_t=f(x_t,h_{t-1},w_h),\\ o_t=g(h_t,w_o),
$$
然后通过一个目标函数在所有$T$个时间步内评估输出$o_t$和对应的标签$y_t$之间的差异：
$$
L(x_1,\ldots,x_T,y_1,\ldots,y_T,w_h,w_o)=\dfrac{1}{T}\sum\limits_{t=1}^T l(y_t,o_t).
$$
对于反向传播，问题则有点棘手， 特别是当我们计算目标函数关于参数的梯度时。 具体来说，按照链式法则：
$$
\begin{aligned}\dfrac{\partial L}{\partial w_h}&=\dfrac{1}{T}\sum_{t=1}^T\dfrac{\partial l(y_t,o_t)}{\partial w_h}\\ &=\dfrac1{T}\sum_t=\dfrac1{t}\dfrac{\partial l(y_{t},o_t)}{\partial o_t}\dfrac{\partial g(h_t,w_v)}{\partial h_t}\dfrac{\partial h_t}{\partial w_h}\end{aligned}
$$
难点在于隐变量对权重的导：
$$
\dfrac{\partial h_t}{\partial w_h}=\dfrac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h}+\dfrac{\partial f({x_t},h_{t-1},{w_h})}{\partial h_{t-1}}\dfrac{\partial h_{t-1}}{\partial w_h}
$$


![image-20230405125419052](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405125419052.png)

最终代换后得到：

![image-20230405125610356](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405125610356.png)

#### 完全计算

我们可以仅仅计算 [(8.7.7)](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-ht-wh-gen)【即上图中的累和】中的全部总和， 然而，这样的计算非常缓慢，并且可能会发生梯度爆炸， 因为初始条件的微小变化就可能会对结果产生巨大的影响。

#### 截断时间步

可以在$\tau$步后截断求和运算。它通常被称为截断的通过时间反向传播 ([Jaeger, 2002](https://zh.d2l.ai/chapter_references/zreferences.html#id77))。 这样做导致该模型主要侧重于短期影响，而不是长期影响。 

#### 随机截断

使用序列$\xi_t$来实现的， 序列预定义了$0\leq\pi_t\leq1$， 其中$P(\xi_t=0)=1-\pi_t$且$P(\xi_t=\pi_t^{-1})=\pi_t$， 因此$E[\xi_t]=1$。 我们使用它来替换 [(8.7.4)](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-ht-wh-recur)中的 梯度$\partial h_t/\partial w_h$得到：
$$
z_{t}=\dfrac{\partial f(x_t,h_{t-1},w_h)}{\partial w_h}+\xi_{t}\dfrac{\partial f(x_{t},h_{t-1},w_{h})}{\partial h_{t-1}}\dfrac{\partial h_{t-1}}{\partial w_h}
$$
从$\xi_t$的定义中推导出来$E[z_t] = \partial h_t/\partial w_h$。每当$\xi_t = 0$时，递归计算终止在这个$t$时间步。这导致了不同长度序列的加权和，其中长序列出现的很少， 所以将适当地加大权重。

#### 策略比较

![image-20230405133257175](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405133257175.png)

虽然随机截断在理论上具有吸引力， 但很可能是由于多种因素在实践中并不比常规截断更好。 首先，在对过去若干个时间步经过反向传播后， 观测结果足以捕获实际的依赖关系。 其次，增加的方差抵消了时间步数越多梯度越精确的事实。 第三，我们真正想要的是只有短范围交互的模型。

 因此，模型需要的正是***“截断”所具备的轻度正则化效果。***

### 反向传播的细节

目标函数的总体损失：
$$
L=\dfrac{1}{T}\sum\limits_{t=1}^T l(\mathbf{o}_t,y_t)
$$


![image-20230405134906446](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405134906446.png)

现在，我们可以计算目标函数关于输出层中参数$W_qh$的梯度:

![image-20230405140602419](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405140602419.png)

接下来：

![image-20230405145024821](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405145024821.png)

![image-20230405145241754](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405145241754.png)

我们可以从 [(8.7.15)](https://zh.d2l.ai/chapter_recurrent-neural-networks/bptt.html#equation-eq-bptt-partial-l-ht)中看到， 这个简单的线性例子已经展现了长序列模型的一些关键问题： 它陷入到$\mathbf{W}_{hh}^\top$的潜在的非常大的幂。

在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。 这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。 

![image-20230405145921531](https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230405145921531.png)

- “通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。
- 截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。
- 矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。
- 为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。
