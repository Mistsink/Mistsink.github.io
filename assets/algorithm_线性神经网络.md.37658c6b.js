import{_ as e,o as a,c as t,a as o}from"./chunks/framework.882288bf.js";const u=JSON.parse('{"title":"线性神经网络","description":"","frontmatter":{"layout":"doc","title":"线性神经网络","createTime":"2023/3/24","preview":"我们从经典算法-线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。"},"headers":[],"relativePath":"algorithm/线性神经网络.md","lastUpdated":1679934595000}'),r={name:"algorithm/线性神经网络.md"},i=o('<h1 id="线性神经网络" tabindex="-1">线性神经网络 <a class="header-anchor" href="#线性神经网络" aria-label="Permalink to &quot;线性神经网络&quot;">​</a></h1><h2 id="线性回归" tabindex="-1">线性回归 <a class="header-anchor" href="#线性回归" aria-label="Permalink to &quot;线性回归&quot;">​</a></h2><blockquote><p><em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。</p></blockquote><h3 id="基本元素" tabindex="-1">基本元素 <a class="header-anchor" href="#基本元素" aria-label="Permalink to &quot;基本元素&quot;">​</a></h3><h4 id="线性模型" tabindex="-1">线性模型 <a class="header-anchor" href="#线性模型" aria-label="Permalink to &quot;线性模型&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324145628037.png" alt="image-20230324145628037"></p><blockquote><p>即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。</p><p>在开始寻找最好的<em>模型参数</em>（model parameters）w和b之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。</p></blockquote><h4 id="损失函数" tabindex="-1">损失函数 <a class="header-anchor" href="#损失函数" aria-label="Permalink to &quot;损失函数&quot;">​</a></h4><p><em>损失函数</em>（loss function）能够量化目标的<em>实际</em>值与<em>预测</em>值之间的差距。</p><p>为了度量模型在整个数据集上的质量，我们需计算在训练集n个样本上的损失均值（也等价于求和）。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324150109654.png" alt="image-20230324150109654"></p><h4 id="解析解" tabindex="-1">解析解 <a class="header-anchor" href="#解析解" aria-label="Permalink to &quot;解析解&quot;">​</a></h4><p>线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）</p><blockquote><p>解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。</p></blockquote><h4 id="随机梯度下降" tabindex="-1">随机梯度下降 <a class="header-anchor" href="#随机梯度下降" aria-label="Permalink to &quot;随机梯度下降&quot;">​</a></h4><p>**<em>梯度下降</em>（gradient descent）**这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。</p><h4 id="单层神经网络" tabindex="-1">单层神经网络 <a class="header-anchor" href="#单层神经网络" aria-label="Permalink to &quot;单层神经网络&quot;">​</a></h4><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230324192345662.png" alt="image-20230324192345662"></p><p>由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 也就是说， [图3.1.2]中神经网络的<em>层数</em>为1。</p><p>对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（ [图3.1.2]中的输出层） 称为<em>全连接层</em>（fully-connected layer）或称为<em>稠密层</em>（dense layer）。</p><h2 id="softmax回归" tabindex="-1"><em><strong>Softmax</strong></em>回归 <a class="header-anchor" href="#softmax回归" aria-label="Permalink to &quot;***Softmax***回归&quot;">​</a></h2><blockquote><p>通常，机器学习实践者用<em>分类</em>这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。</p></blockquote><ul><li>独热编码</li></ul><h3 id="网络架构" tabindex="-1">网络架构 <a class="header-anchor" href="#网络架构" aria-label="Permalink to &quot;网络架构&quot;">​</a></h3><p>为了解决线性模型的分类问题，我们需要和输出一样多的<em>仿射函数</em>（affine function）。 每个输出对应于它自己的仿射函数。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230325235115225.png" alt="image-20230325235115225"></p><h3 id="softmax-运算" tabindex="-1">Softmax 运算 <a class="header-anchor" href="#softmax-运算" aria-label="Permalink to &quot;Softmax 运算&quot;">​</a></h3><p>然而我们能否将未规范化的预测o直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 这些违反了 <a href="https://zh.d2l.ai/chapter_preliminaries/probability.html#sec-prob" target="_blank" rel="noreferrer">2.6节</a>中所说的概率基本公理。</p><p><strong>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 这个属性叫做<em>校准</em>（calibration）。</strong></p><p><em><strong>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。</strong></em></p><p>我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230325235946262.png" alt="image-20230325235946262"></p><blockquote><p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个<em>线性模型</em>（linear model）。</p></blockquote><h3 id="损失函数-1" tabindex="-1">损失函数 <a class="header-anchor" href="#损失函数-1" aria-label="Permalink to &quot;损失函数&quot;">​</a></h3><blockquote><p>最大似然估计</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326001543382.png" alt="image-20230326001543382"></p><h3 id="softmax及其导数" tabindex="-1">Softmax及其导数 <a class="header-anchor" href="#softmax及其导数" aria-label="Permalink to &quot;Softmax及其导数&quot;">​</a></h3><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326002527550.png" alt="image-20230326002527550"></p><blockquote><p>换句话说，<strong>导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值y和估计值y^之间的差异</strong>。 这不是巧合，在任何指数族分布模型中 （参见<a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/distributions.html" target="_blank" rel="noreferrer">本书附录中关于数学分布的一节</a>）， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。</p></blockquote><h3 id="交叉熵损失" tabindex="-1">交叉熵损失 <a class="header-anchor" href="#交叉熵损失" aria-label="Permalink to &quot;交叉熵损失&quot;">​</a></h3><blockquote><p>我们将通过介绍信息论基础来理解交叉熵损失。 如果想了解更多信息论的细节，请进一步参考 <a href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html" target="_blank" rel="noreferrer">本书附录中关于信息论的一节</a>。</p></blockquote><h4 id="信息论" tabindex="-1">信息论 <a class="header-anchor" href="#信息论" aria-label="Permalink to &quot;信息论&quot;">​</a></h4><blockquote><p><em>信息论</em>（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。</p></blockquote><p>信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布P的<em>熵</em>（entropy）。可以通过以下方程得到：</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326002917389.png" alt="image-20230326002917389"></p><p>但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到”惊异”。 克劳德·香农决定用信息量</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230326003434645.png" alt="image-20230326003434645"></p><p>来量化这种惊异程度。 在观察一个事件j时，并赋予它（主观）概率P(j)。 <strong>当我们赋予一个事件较低的概率时，我们的惊异会更大（就是上面那个表达式的值越大），该事件的信息量也就更大（惊异越大就说明我们对事件了解程度很少，相对来说就是事件包含的信息很多）。</strong></p><p><em><strong>所以理解：在 <a href="https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#equation-eq-softmax-reg-entropy" target="_blank" rel="noreferrer">(3.4.11)</a>中定义的熵， 是当分配的概率真正匹配数据生成过程时的信息量的期望。</strong></em></p><h4 id="重新认识交叉熵" tabindex="-1">重新认识交叉熵 <a class="header-anchor" href="#重新认识交叉熵" aria-label="Permalink to &quot;重新认识交叉熵&quot;">​</a></h4><p>如果把熵H(P)想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？ <em><strong>交叉熵从<code>P</code>到<code>Q</code>，记为<code>H(P,Q)</code>。 我们可以把交叉熵想象为“主观概率为<code>Q</code>的观察者在看到根据概率<code>P</code>生成的数据时的预期惊异”。 当<code>Q=P</code>时，交叉熵达到最低。 在这种情况下，从<code>Q</code>到<code>P</code>的交叉熵是<code>H(P,P)=H(P)</code>。</strong></em></p><blockquote><p>简而言之，我们可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。</p></blockquote><h3 id="实践" tabindex="-1">实践 <a class="header-anchor" href="#实践" aria-label="Permalink to &quot;实践&quot;">​</a></h3><p>在PyTorch中，通常将softmax和交叉熵损失函数（CrossEntropyLoss）结合在一起使用，是因为这样可以提高计算效率。***具体而言，CrossEntropyLoss函数在内部已经实现了softmax操作和对数计算，因此无需显式地调用softmax函数。***直接将模型的输出值送入CrossEntropyLoss函数中即可计算softmax及其对数，并且可以避免数值稳定性的问题。</p><p>需要注意的是，如果使用了其他的损失函数，或者需要在模型的输出之后进行其他的操作（例如计算预测精度），则可能需要显式地调用softmax函数。</p>',55),s=[i];function n(m,l,h,c,p,d){return a(),t("div",null,s)}const b=e(r,[["render",n]]);export{u as __pageData,b as default};
