import{_ as s,o as a,c as n,a as l}from"./chunks/framework.13d73f51.js";const d=JSON.parse('{"title":"预备知识","description":"","frontmatter":{"layout":"doc","title":"预备知识","createTime":"2023/3/17"},"headers":[],"relativePath":"algorithm/预备知识.md","lastUpdated":1679068303000}'),p={name:"algorithm/预备知识.md"},o=l(`<h1 id="预备知识" tabindex="-1">预备知识 <a class="header-anchor" href="#预备知识" aria-label="Permalink to &quot;预备知识&quot;">​</a></h1><blockquote><p>包含必备的 Python 操作、线性代数以及简单的数学知识</p></blockquote><h2 id="数据操作" tabindex="-1">数据操作 <a class="header-anchor" href="#数据操作" aria-label="Permalink to &quot;数据操作&quot;">​</a></h2><h3 id="节约内存" tabindex="-1">节约内存 <a class="header-anchor" href="#节约内存" aria-label="Permalink to &quot;节约内存&quot;">​</a></h3><blockquote><p>数据原地操作</p></blockquote><p><code>使用切片表示法将操作的结果分配给先前分配的数组</code></p><p>执行原地操作非常简单。 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如<code>Y[:] = &lt;expression&gt;</code>。 为了说明这一点，我们首先创建一个新的矩阵<code>Z</code>，其形状与另一个<code>Y</code>相同， 使用<code>zeros_like</code>来分配一个全0的块。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">Z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">zeros_like</span><span style="color:#ABB2BF;">(Y)</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#ABB2BF;">Z[:] </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> X </span><span style="color:#56B6C2;">+</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">Z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">zeros_like</span><span style="color:#ABB2BF;">(Y)</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#ABB2BF;">Z[:] </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> X </span><span style="color:#56B6C2;">+</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">print</span><span style="color:#ABB2BF;">(</span><span style="color:#98C379;">&#39;id(Z):&#39;</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(Z): </span><span style="color:#D19A66;">139931132035296</span></span>
<span class="line"></span></code></pre></div><p>如果在后续计算中没有重复使用<code>X</code>， 我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">before </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X)</span></span>
<span class="line"><span style="color:#ABB2BF;">X </span><span style="color:#56B6C2;">+=</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X) </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> before</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#D19A66;">True</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">before </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X)</span></span>
<span class="line"><span style="color:#ABB2BF;">X </span><span style="color:#56B6C2;">+=</span><span style="color:#ABB2BF;"> Y</span></span>
<span class="line"><span style="color:#56B6C2;">id</span><span style="color:#ABB2BF;">(X) </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> before</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#D19A66;">True</span></span>
<span class="line"></span></code></pre></div><h2 id="线性代数" tabindex="-1">线性代数 <a class="header-anchor" href="#线性代数" aria-label="Permalink to &quot;线性代数&quot;">​</a></h2><h3 id="张量算法的基本性质" tabindex="-1">张量算法的基本性质 <a class="header-anchor" href="#张量算法的基本性质" aria-label="Permalink to &quot;张量算法的基本性质&quot;">​</a></h3><h4 id="hadamard-product-⊙" tabindex="-1">Hadamard product ⊙ <a class="header-anchor" href="#hadamard-product-⊙" aria-label="Permalink to &quot;Hadamard product ⊙&quot;">​</a></h4><p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312111525665.png" alt="image-20230312111525665"></p><h4 id="非降维运算-重点在非降维" tabindex="-1">非降维运算（重点在非降维） <a class="header-anchor" href="#非降维运算-重点在非降维" aria-label="Permalink to &quot;非降维运算（重点在非降维）&quot;">​</a></h4><p><code>keepdims</code>参数与<code>广播机制</code></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># 求和</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> A.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">axis</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">1</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">keepdims</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[ </span><span style="color:#D19A66;">6</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">22</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">54</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">70</span><span style="color:#ABB2BF;">.]])</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 归一化</span></span>
<span class="line"><span style="color:#ABB2BF;">A </span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;"> sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[</span><span style="color:#D19A66;">0.0000</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.1667</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3333</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.5000</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.1818</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2273</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2727</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3182</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2105</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2368</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2632</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2895</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2222</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2407</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2593</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2778</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2286</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2429</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2571</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2714</span><span style="color:#ABB2BF;">]])</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># 求和</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> A.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">axis</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">1</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">keepdims</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[ </span><span style="color:#D19A66;">6</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">22</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">54</span><span style="color:#ABB2BF;">.],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">70</span><span style="color:#ABB2BF;">.]])</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 归一化</span></span>
<span class="line"><span style="color:#ABB2BF;">A </span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;"> sum_A</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;">#</span></span>
<span class="line"><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([[</span><span style="color:#D19A66;">0.0000</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.1667</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3333</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.5000</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.1818</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2273</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2727</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.3182</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2105</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2368</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2632</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2895</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2222</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2407</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2593</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2778</span><span style="color:#ABB2BF;">],</span></span>
<span class="line"><span style="color:#ABB2BF;">        [</span><span style="color:#D19A66;">0.2286</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2429</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2571</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">0.2714</span><span style="color:#ABB2BF;">]])</span></span>
<span class="line"></span></code></pre></div><h4 id="点积-dot-product" tabindex="-1">点积（Dot Product） <a class="header-anchor" href="#点积-dot-product" aria-label="Permalink to &quot;点积（Dot Product）&quot;">​</a></h4><p><code>torch.dot()</code></p><p>两向量各元素相乘再相加。</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112320864.png" alt="image-20230312112320864"></p><h4 id="矩阵-向量-积-矩阵-向量" tabindex="-1">矩阵-向量 积（矩阵 * 向量） <a class="header-anchor" href="#矩阵-向量-积-矩阵-向量" aria-label="Permalink to &quot;矩阵-向量 积（矩阵 * 向量）&quot;">​</a></h4><p><code>torch.mv()</code></p><p>矩阵各行向量与向量的点积：</p><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112448685.png" alt="image-20230312112448685"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">A.shape, x.shape, torch.</span><span style="color:#61AFEF;">mv</span><span style="color:#ABB2BF;">(A, x)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># </span></span>
<span class="line"><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">5</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), </span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([ </span><span style="color:#D19A66;">14</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">62</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">86</span><span style="color:#ABB2BF;">., </span><span style="color:#D19A66;">110</span><span style="color:#ABB2BF;">.]))</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">A.shape, x.shape, torch.</span><span style="color:#61AFEF;">mv</span><span style="color:#ABB2BF;">(A, x)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># </span></span>
<span class="line"><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">5</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), torch.</span><span style="color:#61AFEF;">Size</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">]), </span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([ </span><span style="color:#D19A66;">14</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">38</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">62</span><span style="color:#ABB2BF;">.,  </span><span style="color:#D19A66;">86</span><span style="color:#ABB2BF;">., </span><span style="color:#D19A66;">110</span><span style="color:#ABB2BF;">.]))</span></span>
<span class="line"></span></code></pre></div><h4 id="矩阵相乘" tabindex="-1">矩阵相乘 <a class="header-anchor" href="#矩阵相乘" aria-label="Permalink to &quot;矩阵相乘&quot;">​</a></h4><blockquote><p>仅作为与<code>Hadamard 积</code>有区别的提醒</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312112803028.png" alt="image-20230312112803028"></p><h4 id="范数" tabindex="-1">范数 <a class="header-anchor" href="#范数" aria-label="Permalink to &quot;范数&quot;">​</a></h4><blockquote><p>线性代数中最有用的一些运算符是<em>范数</em>（<code>norm</code>）。 非正式地说，向量的<em>范数</em>是表示一个向量有多大。 这里考虑的<em>大小</em>（<code>size</code>）概念不涉及维度，而是分量的大小。</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113045447.png" alt="image-20230312113045447"></p><h5 id="l2范数" tabindex="-1">L2范数 <a class="header-anchor" href="#l2范数" aria-label="Permalink to &quot;L2范数&quot;">​</a></h5><blockquote><p><em>L2范数</em>是向量元素平方和的平方根</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113144939.png" alt="image-20230312113144939"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">3.0</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">-</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">])</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(u)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(5.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">tensor</span><span style="color:#ABB2BF;">([</span><span style="color:#D19A66;">3.0</span><span style="color:#ABB2BF;">, </span><span style="color:#56B6C2;">-</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">])</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(u)</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(5.)</span></span>
<span class="line"></span></code></pre></div><h5 id="l1范数" tabindex="-1">L1范数 <a class="header-anchor" href="#l1范数" aria-label="Permalink to &quot;L1范数&quot;">​</a></h5><blockquote><p><em>L1范数</em>是向量元素的绝对值之和</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113346063.png" alt="image-20230312113346063"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">abs</span><span style="color:#ABB2BF;">(u).</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(7.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">abs</span><span style="color:#ABB2BF;">(u).</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(7.)</span></span>
<span class="line"></span></code></pre></div><h5 id="frobenius范数-针对矩阵" tabindex="-1"><em>Frobenius范数</em>（针对矩阵） <a class="header-anchor" href="#frobenius范数-针对矩阵" aria-label="Permalink to &quot;*Frobenius范数*（针对矩阵）&quot;">​</a></h5><blockquote><p>是矩阵元素平方和的平方根</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312113539564.png" alt="image-20230312113539564"></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">((</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">9</span><span style="color:#ABB2BF;">)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(6.)</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#7F848E;font-style:italic;"># torch.norm()</span></span>
<span class="line"><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">norm</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">((</span><span style="color:#D19A66;">4</span><span style="color:#ABB2BF;">, </span><span style="color:#D19A66;">9</span><span style="color:#ABB2BF;">)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor(6.)</span></span>
<span class="line"></span></code></pre></div><h2 id="微积分" tabindex="-1">微积分 <a class="header-anchor" href="#微积分" aria-label="Permalink to &quot;微积分&quot;">​</a></h2><h3 id="梯度-gradient" tabindex="-1">梯度（gradient） <a class="header-anchor" href="#梯度-gradient" aria-label="Permalink to &quot;梯度（gradient）&quot;">​</a></h3><blockquote><p>就是函数的偏导；对于向量输入而言，偏导的输出也是一个向量：就是偏导向量，称为梯度向量。</p></blockquote><p><img src="https://raw.githubusercontent.com/Mistsink/image-host/main/img/image-20230312114226705.png" alt="image-20230312114226705"></p><h2 id="自动微分" tabindex="-1">自动微分 <a class="header-anchor" href="#自动微分" aria-label="Permalink to &quot;自动微分&quot;">​</a></h2><blockquote><p>实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。</p></blockquote><h3 id="非标量变量的反向传播" tabindex="-1">非标量变量的反向传播 <a class="header-anchor" href="#非标量变量的反向传播" aria-label="Permalink to &quot;非标量变量的反向传播&quot;">​</a></h3><p>在<code>PyTorch</code>中有个简单的规定，不让张量对张量求导，只允许标量对张量求导。因此，目标量对一个非标量调用<code>backward()</code>，则需要传入一个<code>gradient参数</code>。传入这个参数就是为了把张量对张量的求导转换为标量对张量的求导。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">x </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">arange</span><span style="color:#ABB2BF;">(</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">requires_grad</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x  </span><span style="color:#7F848E;font-style:italic;"># y 是向量</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># y.backward()报错</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导数的和，那么给每个分量的梯度是1</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">gradient</span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">(</span><span style="color:#56B6C2;">len</span><span style="color:#ABB2BF;">(x)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数是对张量 y 进行 乘法运算，它就能输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 记 gradient 为张量 w，点积是向量维度的</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 若 y (亦即w) 是更高维度的张量，看作是 y 与 w 在每个维度上做点积，这样便输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 这就是将 对张量的反向传播 =&gt; 对标量的反向传播</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 该例中等价于: y.sum().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导的平均</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones_like</span><span style="color:#ABB2BF;">(x)</span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;">x.</span><span style="color:#61AFEF;">numel</span><span style="color:#ABB2BF;">())</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># === y.mean().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数表征的是 分配不同的权重 给各分量。因为不用的值的梯度对函数结果的影响程度可能不同。</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">x </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> torch.</span><span style="color:#61AFEF;">arange</span><span style="color:#ABB2BF;">(</span><span style="color:#D19A66;">4.0</span><span style="color:#ABB2BF;">, </span><span style="color:#E06C75;font-style:italic;">requires_grad</span><span style="color:#56B6C2;">=</span><span style="color:#D19A66;">True</span><span style="color:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x  </span><span style="color:#7F848E;font-style:italic;"># y 是向量</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># y.backward()报错</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导数的和，那么给每个分量的梯度是1</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;">gradient</span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;">torch.</span><span style="color:#61AFEF;">ones</span><span style="color:#ABB2BF;">(</span><span style="color:#56B6C2;">len</span><span style="color:#ABB2BF;">(x)))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数是对张量 y 进行 乘法运算，它就能输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 记 gradient 为张量 w，点积是向量维度的</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 若 y (亦即w) 是更高维度的张量，看作是 y 与 w 在每个维度上做点积，这样便输出一个标量了</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 这就是将 对张量的反向传播 =&gt; 对标量的反向传播</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 该例中等价于: y.sum().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 例：求偏导的平均</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">(torch.</span><span style="color:#61AFEF;">ones_like</span><span style="color:#ABB2BF;">(x)</span><span style="color:#56B6C2;">/</span><span style="color:#ABB2BF;">x.</span><span style="color:#61AFEF;">numel</span><span style="color:#ABB2BF;">())</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># === y.mean().backward()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># gradient 参数表征的是 分配不同的权重 给各分量。因为不用的值的梯度对函数结果的影响程度可能不同。</span></span>
<span class="line"></span></code></pre></div><h3 id="分离计算" tabindex="-1">分离计算 <a class="header-anchor" href="#分离计算" aria-label="Permalink to &quot;分离计算&quot;">​</a></h3><p><code>[tensor].detach()</code></p><p>将某些计算移动到记录的计算图之外。</p><p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 <strong>换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。</strong></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki one-dark-pro vp-code-dark"><code><span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> y.</span><span style="color:#61AFEF;">detach</span><span style="color:#ABB2BF;">()  </span><span style="color:#7F848E;font-style:italic;"># 分离操作</span></span>
<span class="line"><span style="color:#ABB2BF;">z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> u </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;">z.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> u</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> </span><span style="color:#D19A66;">2</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span></code></pre><pre class="shiki one-dark-pro vp-code-light"><code><span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> x </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#ABB2BF;">u </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> y.</span><span style="color:#61AFEF;">detach</span><span style="color:#ABB2BF;">()  </span><span style="color:#7F848E;font-style:italic;"># 分离操作</span></span>
<span class="line"><span style="color:#ABB2BF;">z </span><span style="color:#56B6C2;">=</span><span style="color:#ABB2BF;"> u </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;">z.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> u</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># 由于记录了y的计算结果，我们可以随后在y上调用反向传播， 得到y=x*x关于的x的导数，即2*x。</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad.</span><span style="color:#61AFEF;">zero_</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">y.</span><span style="color:#61AFEF;">sum</span><span style="color:#ABB2BF;">().</span><span style="color:#61AFEF;">backward</span><span style="color:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;">x.grad </span><span style="color:#56B6C2;">==</span><span style="color:#ABB2BF;"> </span><span style="color:#D19A66;">2</span><span style="color:#ABB2BF;"> </span><span style="color:#56B6C2;">*</span><span style="color:#ABB2BF;"> x</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;"># tensor([True, True, True, True])</span></span>
<span class="line"></span></code></pre></div>`,59),e=[o];function t(c,r,B,y,i,F){return a(),n("div",null,e)}const h=s(p,[["render",t]]);export{d as __pageData,h as default};
